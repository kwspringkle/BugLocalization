{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnDrKfrC1Q2dipDftHmBkp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Bug Localization - DNN**"
      ],
      "metadata": {
        "id": "vCDlOi0xvMHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cfBzQkcvN_N",
        "outputId": "b289a60c-bc2e-46cf-931a-333230f927a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = 'drive/MyDrive/Colab Notebooks/BugLocalization'\n",
        "\n",
        "import sys\n",
        "sys.path.append(project_path)"
      ],
      "metadata": {
        "id": "NpqI0hXevPNO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tiền xử lý dữ liệu"
      ],
      "metadata": {
        "id": "kGwNUm-8vyfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle\n",
        "!pip install inflection\n",
        "!pip install nltk\n",
        "!pip install javalang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avsWi_m6vx_b",
        "outputId": "7beaa752-fb9e-40ba-c07c-8ef565387fa7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting inflection\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: inflection\n",
            "Successfully installed inflection-0.5.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting javalang\n",
            "  Downloading javalang-0.13.0-py3-none-any.whl.metadata (805 bytes)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from javalang) (1.17.0)\n",
            "Downloading javalang-0.13.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: javalang\n",
            "Successfully installed javalang-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import Parser, ReportPreprocessing, SrcPreprocessing\n",
        "from projectDatasets import aspectj, eclipse, swt, tomcat, birt\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxkloOn5v1Dv",
        "outputId": "2834d0d3-f080-462c-cd52-45a6243dfa43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Đường dẫn thư mục models trong dự án\n",
        "models_dir = f\"{project_path}/models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "bug_reports_path = f\"{models_dir}/preprocessed_bug_reports.pkl\"\n",
        "src_reports_path = f\"{models_dir}/preprocessed_src_reports.pkl\"\n",
        "\n",
        "with open(src_reports_path, \"rb\") as f:\n",
        "    aspectj_src_files = pickle.load(f)\n",
        "    eclipse_src_files = pickle.load(f)\n",
        "    swt_src_files = pickle.load(f)\n",
        "    tomcat_src_files = pickle.load(f)\n",
        "    birt_src_files = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(bug_reports_path, \"rb\") as f:\n",
        "    aspectj_bug_reports = pickle.load(f)\n",
        "    eclipse_bug_reports = pickle.load(f)\n",
        "    swt_bug_reports = pickle.load(f)\n",
        "    tomcat_bug_reports = pickle.load(f)\n",
        "    birt_bug_reports = pickle.load(f)\n",
        "\n",
        "print(\"Data load successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHruwQegv4MT",
        "outputId": "36686099-3a91-41b5-8e5f-51a24ce5356d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data load successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chia fold, gán nhãn dữ liệu"
      ],
      "metadata": {
        "id": "cIMVaAXHv_mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chia fold cho bug reports\n",
        "- 3 fold cho Aspectj\n",
        "- 10 fold cho các projects còn lại\n",
        "- Sắp xếp bug reports rồi chia theo thời gian từ cũ đến mới (fold i để test và fold i + 1 để train)"
      ],
      "metadata": {
        "id": "GZr0ak66Rzjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sắp xếp lại bug reports theo thời gian từ cũ đến mới\n",
        "sorted_aspectj_bug_reports = sorted(aspectj_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_eclipse_bug_reports = sorted(eclipse_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_swt_bug_reports = sorted(swt_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_tomcat_bug_reports = sorted(tomcat_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_birt_bug_reports = sorted(birt_bug_reports.items(), key=lambda x: x[1].report_time)"
      ],
      "metadata": {
        "id": "UM-L7BjbRzPd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy một báo cáo lỗi cụ thể, ví dụ báo cáo lỗi đầu tiên trong danh sách\n",
        "bug_report = sorted_aspectj_bug_reports[0]\n",
        "bug_report2 = sorted_aspectj_bug_reports[1]\n",
        "\n",
        "# Truy cập vào report_time của bug report\n",
        "report_time = bug_report[1].report_time  # bug_report[1] là đối tượng BugReport\n",
        "report_time2 = bug_report2[1].report_time\n",
        "\n",
        "# In ra thời gian báo cáo, kiểm tra xem đúng thứ tự từ cũ đến mới chưa\n",
        "print(report_time)\n",
        "print(report_time2)\n",
        "\n",
        "print(len(sorted_aspectj_bug_reports))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZTGrYzHSF7e",
        "outputId": "fc9db4b0-c749-4834-8216-50245aa8a273"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2002-03-13 12:40:54\n",
            "2002-12-30 16:40:03\n",
            "593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Chia fold theo thứ tự thời gian\n",
        "def split_time_ordered_folds(sorted_data, n_folds):\n",
        "    fold_size = len(sorted_data) // n_folds # Tính xấp xỉ fold_size cần\n",
        "    folds = []\n",
        "\n",
        "    # Tạo các indices cho từng fold\n",
        "    indices = np.arange(len(sorted_data)) # Tạo mảng chỉ số cho các phần tử reports\n",
        "    fold_indices = np.array_split(indices, n_folds) # Chia mảng indices thành n folds, kích thước bằng nhau nhất có thể\n",
        "\n",
        "    # Lưu các fold và indices của chúng\n",
        "    fold_data = []\n",
        "    for fold_idx in fold_indices:\n",
        "        fold_data.append([sorted_data[i] for i in fold_idx])\n",
        "        # Thêm data vào fold data nếu chỉ số có ở trong tập fold_indices\n",
        "\n",
        "    return fold_data\n",
        "\n",
        "# Chia thành các fold theo yêu cầu\n",
        "\n",
        "# AspectJ chia 3 fold\n",
        "aspectj_folds = split_time_ordered_folds(sorted_aspectj_bug_reports, 3)\n",
        " # Các project khác chia 10 fold\n",
        "eclipse_folds = split_time_ordered_folds(sorted_eclipse_bug_reports, 10)\n",
        "swt_folds = split_time_ordered_folds(sorted_swt_bug_reports, 10)\n",
        "tomcat_folds = split_time_ordered_folds(sorted_tomcat_bug_reports, 10)\n",
        "birt_folds = split_time_ordered_folds(sorted_birt_bug_reports, 10)"
      ],
      "metadata": {
        "id": "T9RU9lYcSPvh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gán nhãn dữ liệu\n",
        "- Nếu liên quan --> label là 1\n",
        "- Nếu không liên quan --> label là 0\n",
        "- Vấn đề: label 0 sẽ có rất nhiều, trong khi label 1 rất ít (skewed distribution --> cách giải quyết: data-sample manipulation, cost-sensitive learning(focal-loss function))\n",
        "  - Giải quyết bài toán: Chọn ra top k các lớp 0 có similarity lớn, khó phân biệt với lớp 1 để train(hard negative sampling)"
      ],
      "metadata": {
        "id": "cP6GbdW1WPMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm tính cosine similarity giữa 2 văn bản (lexical similarity)"
      ],
      "metadata": {
        "id": "iVCm8iyZbmkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bug_report_to_text(bug_report):\n",
        "    text_parts = []\n",
        "\n",
        "    # Kiểm tra và thêm phần tóm tắt đã gắn nhãn từ bug_report\n",
        "    if bug_report.pos_tagged_summary is not None and 'unstemmed' in bug_report.pos_tagged_summary:\n",
        "        text_parts.append(\" \".join(bug_report.pos_tagged_summary['unstemmed']))\n",
        "\n",
        "    # Kiểm tra và thêm phần mô tả đã gắn nhãn từ bug_report\n",
        "    if bug_report.pos_tagged_description is not None and 'unstemmed' in bug_report.pos_tagged_description:\n",
        "        text_parts.append(\" \".join(bug_report.pos_tagged_description['unstemmed']))\n",
        "\n",
        "    # Kết hợp tất cả các phần và trả về kết quả\n",
        "    text = \" \".join(text_parts)\n",
        "    return text\n",
        "\n",
        "def src_report_to_text(src_report):\n",
        "    text_parts = []\n",
        "\n",
        "    # Kiểm tra và thêm phần bình luận đã gắn nhãn từ source_report\n",
        "    if src_report.pos_tagged_comments is not None and 'unstemmed' in src_report.pos_tagged_comments:\n",
        "        text_parts.append(\" \".join(src_report.pos_tagged_comments['unstemmed']))\n",
        "\n",
        "    # Kiểm tra và thêm tên lớp từ source_report\n",
        "    if src_report.class_names is not None:\n",
        "        text_parts.append(\" \".join(src_report.class_names))\n",
        "\n",
        "    # Kiểm tra và thêm các thuộc tính từ source_report\n",
        "    if src_report.attributes is not None:\n",
        "        text_parts.append(\" \".join(src_report.attributes))\n",
        "\n",
        "    # Kiểm tra và thêm tên phương thức từ source_report\n",
        "    if src_report.method_names is not None:\n",
        "        text_parts.append(\" \".join(src_report.method_names))\n",
        "\n",
        "    # Kiểm tra và thêm biến từ source_report\n",
        "    if src_report.variables is not None:\n",
        "        text_parts.append(\" \".join(src_report.variables))\n",
        "\n",
        "    # Kết hợp tất cả các phần và trả về kết quả\n",
        "    text = \" \".join(text_parts)\n",
        "    return text"
      ],
      "metadata": {
        "id": "IqCrXsGvb7MI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Đầu vào: 2 text (bug report, src_file)\n",
        "def lexical_similarity(bug_report, src_report):\n",
        "    # Chuyển bug_report và src_report thành văn bản\n",
        "    bug_report_text = bug_report_to_text(bug_report)\n",
        "    src_report_text = src_report_to_text(src_report)\n",
        "\n",
        "    # Kết hợp văn bản của bug_report và src_report\n",
        "    all_texts = [bug_report_text, src_report_text]\n",
        "\n",
        "    # Tính TF-IDF cho văn bản\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Lấy các vector TF-IDF cho bug_report và src_report\n",
        "    bug_report_tfidf = tfidf_matrix[0]  # Vector của bug_report\n",
        "    src_report_tfidf = tfidf_matrix[1]  # Vector của src_report\n",
        "\n",
        "    # Tính cosine similarity giữa bug_report và src_report\n",
        "    similarity = cosine_similarity(bug_report_tfidf, src_report_tfidf)\n",
        "\n",
        "    return similarity[0][0]"
      ],
      "metadata": {
        "id": "UC0rTct3b0jm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lấy ra tập dương tính"
      ],
      "metadata": {
        "id": "f2xM38-ecQ4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đầu vào: 1 bug report, tập source files của project đó\n",
        "# Đầu ra: các source files liên quan đến bug report đó\n",
        "\n",
        "def src_positive(bug_report, src_files):\n",
        "  src_pos = []\n",
        "  bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        ")\n",
        "  for src_address, src_file in src_files.items():\n",
        "    if src_address in bug_report_fixed_files:\n",
        "      src_pos.append(src_file)\n",
        "  return src_pos #[src_file, src_file,....]"
      ],
      "metadata": {
        "id": "9fA9m7AlcTrM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lấy ra tập âm tính\n"
      ],
      "metadata": {
        "id": "_wOZV2HAdD-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đầu vào: 1 bug report, tập source files của project đó\n",
        "# Đầu ra: top 500 file không liên quan đến bug report nhưng có lexical similarity cao nhất\n",
        "\n",
        "def src_negative(bug_report, src_files, k=500):\n",
        "  src_neg = []\n",
        "  bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        "  )\n",
        "  sim_scores = []\n",
        "  for src_address , src_file in src_files.items():\n",
        "    if src_address not in bug_report_fixed_files: # Kiểm tra không ở trong tập positive\n",
        "      sim_score = lexical_similarity(bug_report, src_file) # Tính cosine similarity giữa 2 file\n",
        "      sim_scores.append((src_file, sim_score))\n",
        "  sim_scores.sort(key=lambda x: x[1], reverse=True) # Sắp xếp theo phần tử thứ 2 (sim_score), reverse=True --> giảm dần theo sim_score\n",
        "  # Chọn ra top k src file âm tính có sim_score cao nhất\n",
        "  for i in range(k):\n",
        "    src_neg.append(sim_scores[i][0])\n",
        "  return src_neg # [src_file, src_file,...]"
      ],
      "metadata": {
        "id": "O2LwlE3QdF83"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label data"
      ],
      "metadata": {
        "id": "CgD26wZsfGAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(aspectj_folds[0][0][1].fixed_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyeg2MgClopR",
        "outputId": "cad42a35-6059-4909-9f5e-a347b506aa16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tests/src/org/aspectj/systemtest/ajc150/Ajc150Tests.java', ' weaver/src/org/aspectj/weaver/reflect/ReflectionBasedReferenceTypeDelegateFactory.java', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Đầu vào: 1 fold bug_reports dùng để train, src_files của project đó\n",
        "# Đầu ra: 1 list các tuple (bug_report, src_file, label)\n",
        "\n",
        "# 1 fold bug_reports có dạng [(bug_id, BugReport Object),....]\n",
        "\n",
        "def label_data_train(prj_bug_reports_fold, src_files):\n",
        "  data_train = []\n",
        "  for bug_id, bug_report in prj_bug_reports_fold:\n",
        "    src_pos = src_positive(bug_report, src_files)\n",
        "    src_neg = src_negative(bug_report, src_files)\n",
        "    for src_file in src_pos:\n",
        "      data_train.append((bug_report, src_file, 1))\n",
        "    for src_file in src_neg:\n",
        "      data_train.append((bug_report, src_file, 0))\n",
        "  return data_train, src_pos, src_neg"
      ],
      "metadata": {
        "id": "82hkxB8-fJW4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đầu vào: 1 fold bug_reports dùng để test, src_files của project đó\n",
        "# Đầu ra: 1 list các tuple (bug_report, src_file, label)\n",
        "\n",
        "def label_data_test(prj_bug_reports_fold, src_files):\n",
        "  data_test = []\n",
        "  for bug_id, bug_report in prj_bug_reports_fold:\n",
        "    bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        "    )\n",
        "    for src_address, src_file in src_files.items():\n",
        "      if src_address in bug_report_fixed_files:\n",
        "        data_test.append((bug_report, src_file, 1))\n",
        "      else:\n",
        "        data_test.append((bug_report, src_file, 0))\n",
        "  return data_test"
      ],
      "metadata": {
        "id": "FfwJFOOghhRe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label và lưu lại dữ liệu train, test"
      ],
      "metadata": {
        "id": "kApTXU47yU4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_fold_data(project_name, project_src_files, project_folds):\n",
        "    project_dir = f\"{models_dir}/{project_name}\"\n",
        "    print(f\"Saving project: {project_name}\")\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "    n_folds = len(project_folds)\n",
        "\n",
        "    for i in range(n_folds - 1):  # fold i dùng để train, fold i+1 để test\n",
        "        # Gán dữ liệu train/test\n",
        "        data_train, src_pos, src_neg = label_data_train(project_folds[i], project_src_files) #Train fold i\n",
        "        data_test = label_data_test(project_folds[i + 1], project_src_files) # Test fold i + 1\n",
        "\n",
        "        # Lưu bằng pickle\n",
        "        with open(os.path.join(project_dir, f\"fold_{i}_train.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(data_train, f)\n",
        "\n",
        "        with open(os.path.join(project_dir, f\"fold_{i}_test.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(data_test, f)\n",
        "\n",
        "        print(f\"Saved fold {i}: train={len(data_train)}, test={len(data_test)}\")"
      ],
      "metadata": {
        "id": "glY2_3ZE0AhK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_fold_data ('aspectj', aspectj_src_files, aspectj_folds)\n",
        "save_fold_data ('eclipse', eclipse_src_files, eclipse_folds)\n",
        "save_fold_data ('swt', swt_src_files, swt_folds)\n",
        "save_fold_data ('tomcat', tomcat_src_files, tomcat_folds)\n",
        "save_fold_data('birt', birt_src_files, birt_folds)"
      ],
      "metadata": {
        "id": "L3uYlscFSk81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fold_data(project_name, fold_idx):\n",
        "    project_dir = f\"{models_dir}/{project_name}\"\n",
        "\n",
        "    train_file = os.path.join(project_dir, f\"fold_{fold_idx}_train.pkl\")\n",
        "    test_file = os.path.join(project_dir, f\"fold_{fold_idx}_test.pkl\")\n",
        "    src_pos_file = os.path.join(project_dir, f\"fold_{fold_idx}_src_pos.pkl\")\n",
        "    src_neg_file = os.path.join(project_dir, f\"fold_{fold_idx}_src_neg.pkl\")\n",
        "\n",
        "    # Load tập train, tập test\n",
        "    with open(train_file, \"rb\") as f:\n",
        "        data_train = pickle.load(f)\n",
        "    with open(test_file, \"rb\") as f:\n",
        "        data_test = pickle.load(f)\n",
        "    return data_train, data_test"
      ],
      "metadata": {
        "id": "IA8dG0_kTPjN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fold 0 của project aspectj để kiểm tra\n",
        "data_train, data_test  = load_fold_data('aspectj', 0)\n",
        "print(f\"Train data size: {len(data_train)}, Test data size: {len(data_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z0mNXxOSugF",
        "outputId": "d871062d-a5b9-4755-c0dd-6a04b203e6e5"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 99523, Test data size: 929808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "count_label_1 = 0\n",
        "for report, file, label in data_train:\n",
        "  count += 1\n",
        "  if label == 1:\n",
        "    count_label_1 += 1\n",
        "    #print(f\"{report}, {file}, {label}\")\n",
        "print(f\"Label 1: {count_label_1}, Label 0: {count - count_label_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jPodvNmhEsT",
        "outputId": "49d24e5f-1c02-41d3-a69a-a71d980e6aba"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 1: 523, Label 0: 99000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lấy ra 1 phần của test set aspectj\n",
        "import random\n",
        "def stratified(data_test, sample_size, min_label_1 = 1):\n",
        "    data_label_1 = [d for d in data_test if d[2] == 1]\n",
        "    data_label_0 = [d for d in data_test if d[2] == 0]\n",
        "\n",
        "    total = len(data_test)\n",
        "    ratio_label_1 = len(data_label_1) / total\n",
        "\n",
        "    # Tính số mẫu cần lấy\n",
        "    num_label_1 = max(int(sample_size * ratio_label_1), min_label_1)\n",
        "    num_label_1 = min(num_label_1, len(data_label_1))  # không vượt quá số thực tế\n",
        "    num_label_0 = sample_size - num_label_1\n",
        "\n",
        "    sampled_label_1 = random.sample(data_label_1, num_label_1)\n",
        "    sampled_label_0 = random.sample(data_label_0, num_label_0)\n",
        "\n",
        "    sampled_data = sampled_label_1 + sampled_label_0\n",
        "    random.shuffle(sampled_data)\n",
        "    return sampled_data"
      ],
      "metadata": {
        "id": "WhoZgQafb_H0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Trích xuất đặc trưng"
      ],
      "metadata": {
        "id": "3wFg2zJHvNMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Lexical similarity\n",
        "\n",
        "- Hàm bug2text và src2text ở trên"
      ],
      "metadata": {
        "id": "ABy1gaQwvQVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đầu vào: 1 pair trong tập data train (1 tuple (bug_report, source_file, label))\n",
        "# Đầu ra: lexical similarity của pair đó\n",
        "\n",
        "def get_lexical_similarity(pair):\n",
        "    similarity = lexical_similarity(pair[0], pair[1])\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "eT3UhEgovwl-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_lexical_similarity(data_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUL6tAvknOX9",
        "outputId": "31d32f5b-2d8d-4c09-c8d5-6bcad7c79be7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.24505498492568153)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Semantic similarity\n",
        "Do có lexical mismatch khi bug reports sử dụng ngôn ngữ tự nhiên, còn source code lại sử dụng ngôn ngữ lập trình --> Tìm hiểu thêm về semantic relationship giữa các từ phụ thuộc vào context nó xuất hiện\n",
        "\n",
        "- Sử dụng pretrained Glove để chuyển từ --> vectors\n",
        "  - Glove: word embeddings, dựa trên co-occurrence matrix của các từ trong văn bản. Glove sẽ tìm 1 bộ các vector sao cho tần suất đồng xuất hiện của các từ trong không gian vector gần với tần suất đồng xuất hiện trong co-occurence matrix"
      ],
      "metadata": {
        "id": "8xrpJ8u9wK9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained glove\n",
        "glove_path = '/content/drive/MyDrive/Colab Notebooks/BugLocalization/glove.6B/glove.6B.100d.txt'\n",
        "\n",
        "def load_glove_model(glove_file):\n",
        "    print(\"Loading GloVe model...\")\n",
        "    model = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            word = parts[0]\n",
        "            vector = np.asarray(parts[1:], dtype='float32')\n",
        "            model[word] = vector\n",
        "    print(f\"Model loaded with {len(model)} words.\")\n",
        "    return model\n",
        "\n",
        "glove_model = load_glove_model(glove_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA_Me6PowNmm",
        "outputId": "a22a3248-3f45-4f88-d9d6-e997dc97b80f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe model...\n",
            "Model loaded with 400000 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def embed_text_tensor(text, glove_dict, tfidf_weights, vocab, dim=100):\n",
        "    tokens = text.split()\n",
        "    vecs = []\n",
        "    weights = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in glove_dict and token in vocab:\n",
        "            vecs.append(torch.tensor(glove_dict[token]))\n",
        "            weights.append(tfidf_weights.get(token, 0))  # Lấy TF-IDF weight nếu có, nếu không thì 0\n",
        "\n",
        "    if not vecs:\n",
        "        return torch.zeros(dim)\n",
        "\n",
        "    vecs = torch.stack(vecs)  # (n_tokens, dim)\n",
        "    weights = torch.tensor(weights).unsqueeze(1)  # (n_tokens, 1)\n",
        "\n",
        "    # Nhân từng vector với trọng số TF-IDF\n",
        "    weighted_vecs = vecs * weights\n",
        "\n",
        "    # Tính tổng và chuẩn hóa bằng cách chia cho tổng trọng số\n",
        "    return weighted_vecs.sum(dim=0) / weights.sum()"
      ],
      "metadata": {
        "id": "mb2cDWgjwOlu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_semantic_similarity(pair,glove_dict, dim=100, device=\"cpu\"):\n",
        "    # Chuyển bug_report và src_report thành văn bản\n",
        "    bug_report = pair[0]\n",
        "    src_report= pair[1]\n",
        "    bug_report_text = bug_report_to_text(bug_report)  # 1 bug report\n",
        "    src_report_text = src_report_to_text(src_report)  # 1 source report\n",
        "\n",
        "    # Fit TF-IDF để lấy idf weight chỉ cho cặp bug_report và src_report\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf.fit([bug_report_text, src_report_text])\n",
        "    vocab = tfidf.vocabulary_\n",
        "    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
        "\n",
        "    # Embed text thành tensor\n",
        "    def embed_text_tensor_with_tfidf(text):\n",
        "        return embed_text_tensor(text, glove_dict, idf_weights, vocab, dim)\n",
        "\n",
        "    # Embed bug_report và src_report thành vectors\n",
        "    bug_vec = embed_text_tensor_with_tfidf(bug_report_text).to(device)\n",
        "    src_vec = embed_text_tensor_with_tfidf(src_report_text).to(device)\n",
        "\n",
        "    # Tính cosine similarity giữa bug_report và src_report\n",
        "    similarity = cosine_similarity([bug_vec.cpu().numpy()], [src_vec.cpu().numpy()])\n",
        "\n",
        "    return similarity[0][0]  # Trả về giá trị similarity duy nhất"
      ],
      "metadata": {
        "id": "LVOxpwBKwPnF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_semantic_similarity(data_train[0], glove_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1jxthwsnKrj",
        "outputId": "a8e75c07-164a-4935-f8d5-05438f21d773"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.860221418588926)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Similar Bug Reports  \n",
        "Dựa trên các bug reports trước đó mà source file đã được fix, xem xem có giống các bug report đã từng sửa cùng file đó không"
      ],
      "metadata": {
        "id": "Omxo-3yfwjF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_source_files_for_bug_report(target_bug_report, dataset):\n",
        "    # Tìm tất cả các SourceFile liên quan đến BugReport\n",
        "    source_files = [source_file for bug_report, source_file, label in dataset if bug_report == target_bug_report]\n",
        "    return source_files"
      ],
      "metadata": {
        "id": "aQF0v-GQfe5a"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_src_files = find_source_files_for_bug_report(data_train[2001][0], data_train)"
      ],
      "metadata": {
        "id": "VpJzPhH_hZ6I"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_bug_reports_fix_source_files(target_source_files, dataset):\n",
        "    # Tìm tất cả BugReport đã fix SourceFiles (Bug Report đã sửa file đó)(label = 1)\n",
        "    bug_reports = [bug_report for bug_report, source_file, label in dataset if source_file in target_source_files and label == 1]\n",
        "    return bug_reports"
      ],
      "metadata": {
        "id": "HAPCWPHNhUgt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(find_bug_reports_fix_source_files(test_src_files, data_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap5envP9igBj",
        "outputId": "37d42c88-e5bb-4252-ba48-fba363b2073c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_bug_reports(bug_report, dataset):\n",
        "  source_files = find_source_files_for_bug_report(bug_report, dataset)\n",
        "  bug_reports = find_bug_reports_fix_source_files(source_files, dataset)\n",
        "  return bug_reports"
      ],
      "metadata": {
        "id": "OHm3EXXdj_8O"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_bugs_score(pair, dataset):\n",
        "  #pair (BugReport, SrcFile, Label)\n",
        "  previous_bug_reports = get_previous_bug_reports(pair[0], dataset)\n",
        "  if len(previous_bug_reports) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    sum = 0\n",
        "    for bug_report in previous_bug_reports:\n",
        "      bug_report_text = bug_report_to_text(pair[0])\n",
        "      bug2_report_text = bug_report_to_text(bug_report)\n",
        "      # Kết hợp văn bản của bug_report và bug2_report\n",
        "      all_texts = [bug_report_text, bug2_report_text]\n",
        "\n",
        "      # Tính TF-IDF cho văn bản\n",
        "      vectorizer = TfidfVectorizer()\n",
        "      tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "      # Lấy các vector TF-IDF cho bug_report và bug2_report\n",
        "      bug_report_tfidf = tfidf_matrix[0]  # Vector của bug_report\n",
        "      bug2_report_tfidf = tfidf_matrix[1]  # Vector của bug2_report\n",
        "\n",
        "      # Tính cosine similarity giữa bug_report và bug2_report\n",
        "      similarity = cosine_similarity(bug_report_tfidf, bug2_report_tfidf)\n",
        "      sum += similarity[0][0]\n",
        "\n",
        "    return sum / len(previous_bug_reports)"
      ],
      "metadata": {
        "id": "ECgFOAv9kxER"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_bugs_score(data_train[2001], data_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwX5cREFnFpn",
        "outputId": "b80a8ce8-a535-4d0f-c48f-a9ca9fa2e237"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.04978423557773192)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Code change history\n",
        "File mà đã fix rất lâu về trước hoặc chưa bao giờ fix thì sẽ ít khả năng lỗi hơn\n",
        "- Trả về 1 / khoảng cách ngày tới lần fix gần nhất\n",
        " --> Nếu như khoảng cách càng xa, score càng bé"
      ],
      "metadata": {
        "id": "uw47Eyu1nkEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_code_change_history_score(pair, dataset):\n",
        "  report_time = pair[0].report_time\n",
        "  # Trả về tất cả các bug reports mà đã fix source file\n",
        "  fixed_bug_reports_for_src_file = [bug_report for bug_report, source_file, label in dataset if source_file == pair[1] and label == 1]\n",
        "  min_time = report_time\n",
        "  # Lấy report time của các bug_reports đã fix xong, nếu nhỏ hơn report time thì gán min time vào\n",
        "  for bug_report in fixed_bug_reports_for_src_file:\n",
        "    if bug_report.report_time < report_time:\n",
        "      min_time = min(min_time, bug_report.report_time)\n",
        "\n",
        "  elapsed_days = (report_time - min_time).days\n",
        "  if elapsed_days == 0:\n",
        "    return 1 #Nếu trả về 1 thì tức là file đó chưa từng được fix lần nào trước report time của bug_report\n",
        "  else:\n",
        "    return 1 / elapsed_days"
      ],
      "metadata": {
        "id": "qRPd5ddCo3Bw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_code_change_history_score(data_train[2001], data_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHa9IItjGNmr",
        "outputId": "b4fd5894-146f-4ee8-bf5a-b9821cebb2ac"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Bug fixing frequency\n",
        "Số lần mà source file được sửa trước report time cũng sẽ ảnh hưởng đến xác suất file đó có lỗi hay không  \n",
        "(Nếu như sửa càng nhiều --> xác suất lỗi càng cao)"
      ],
      "metadata": {
        "id": "B8-rrsTxGWfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bug_fixing_freq_score(pair, dataset):\n",
        "  report_time = pair[0].report_time\n",
        "  # Trả về tất cả các bug reports mà đã fix source file\n",
        "  fixed_bug_reports_for_src_file = [bug_report for bug_report, source_file, label in dataset\n",
        "                                    if source_file == pair[1] and label == 1]\n",
        "  # Tìm trong các bug_report đã fix nếu report time nhỏ hơn bug_report hiện tại (hay đã fix trước đó)\n",
        "  count = 0\n",
        "  for bug_report in fixed_bug_reports_for_src_file:\n",
        "    if bug_report.report_time < report_time:\n",
        "      count +=1\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "hON3jKglGlqK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_bug_fixing_freq_score(data_train[2001], data_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9mjafZlHcS3",
        "outputId": "c47422c8-67c4-4561-846b-702e5ad3609b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tạo tập train và tập test"
      ],
      "metadata": {
        "id": "DZOHcw_oHpwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gộp các features đã tính\n",
        "Gộp các features đã tính thành 1 vector `[lex_sim, sem_sim, prev_bug_sim, change_history, change_freq]`"
      ],
      "metadata": {
        "id": "BPZ4lYC2Hz7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(pair, dataset, glove_dict):\n",
        "    lex_sim = get_lexical_similarity(pair)\n",
        "    sem_sim = get_semantic_similarity(pair, glove_dict)\n",
        "    prev_bug_sim = get_similar_bugs_score(pair, dataset)\n",
        "    change_history = get_code_change_history_score(pair, dataset)\n",
        "    change_freq = get_bug_fixing_freq_score(pair, dataset)\n",
        "\n",
        "    features_vector = np.array([lex_sim, sem_sim, prev_bug_sim, change_history, change_freq])\n",
        "    return features_vector"
      ],
      "metadata": {
        "id": "BHv7bC6iHy22"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_features(data_train[0], data_train, glove_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXZYqTDLIbAC",
        "outputId": "4c730ea1-4961-4381-d01b-df66c3c30c7f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.24505498, 0.86022142, 0.02084132, 1.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tạo minibatch cho dữ liệu train bằng Bootstraping\n",
        "\n"
      ],
      "metadata": {
        "id": "dboyu69lhZP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm dùng để tạo data: data có dạng [(vector, label),....]; dùng cho cả tập train và tập test\n",
        "# Tập train: fold i; tập test: fold i + 1\n",
        "def create_data(dataset, glove_dict):\n",
        "  print(f\"Creating data for {len(dataset)} samples\")\n",
        "  data = []\n",
        "  for pair in dataset:  #pair: (bug_report, src_file, label) --> label: pair[2]\n",
        "    x_pair = extract_features(pair, dataset, glove_dict)\n",
        "    y_pair = pair[2]\n",
        "    data.append((x_pair, y_pair))\n",
        "  return data"
      ],
      "metadata": {
        "id": "PYccBBDpJCTP"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy ra tập (vector, label) positive\n",
        "def get_pos(data):\n",
        "  pos_set = []\n",
        "  for vector, label in data:\n",
        "    if label == 1:\n",
        "      pos_set.append((vector, 1))\n",
        "  return pos_set\n",
        "\n",
        "# Lấy ra tập (vector, label) negative\n",
        "def get_neg(data):\n",
        "  neg_set = []\n",
        "  for vector, label in data:\n",
        "    if label == 0:\n",
        "      neg_set.append((vector, 0))\n",
        "  return neg_set"
      ],
      "metadata": {
        "id": "zacDLg86LIJM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def create_minibatch(data, batch_size):\n",
        "  print(f\"Creating minibatch for {len(data)} samples\")\n",
        "  mini_batches = []\n",
        "  pos_set = get_pos(data) #[(X_pos, 1)]\n",
        "  neg_set = get_neg(data) #[(X_neg, 0)]\n",
        "\n",
        "  n_pos = len(pos_set)\n",
        "  n_neg = len(neg_set)\n",
        "  K = n_neg // batch_size\n",
        "\n",
        "  Sn = batch_size // 2\n",
        "  Sp = batch_size - Sn\n",
        "\n",
        "  #Chia negative thành K phần, từng phần có Sn samples\n",
        "  neg_batches = [neg_set[i * Sn : (i + 1) * Sn] for i in range(K)]\n",
        "\n",
        "  #Copy tập postive để rút ngẫu nhiên\n",
        "  temp_pos = pos_set.copy()\n",
        "\n",
        "  for i in range(K):\n",
        "    if len(temp_pos) < Sp:\n",
        "      temp_pos = pos_set.copy()\n",
        "\n",
        "    random.shuffle(temp_pos)\n",
        "    #pos_i = random.sample(temp_pos, Sp)\n",
        "    pos_i = random.choices(temp_pos, k=Sp)\n",
        "\n",
        "    #for item in pos_i:\n",
        "    #  temp_pos.remove(item)\n",
        "\n",
        "\n",
        "\n",
        "    #batch = neg_batches[i] + pos_i\n",
        "    #random.shuffle(batch)\n",
        "    batch_inputs = [x for x, y in neg_batches[i] + pos_i]\n",
        "    batch_labels = [y for x, y in neg_batches[i] + pos_i]\n",
        "\n",
        "    mini_batches.append((batch_inputs, batch_labels))\n",
        "  return mini_batches # 1 batch[(X,y),...]; 1 minibatch: list các batch"
      ],
      "metadata": {
        "id": "hMxRUgiYI15K"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Huấn luyện và đánh giá"
      ],
      "metadata": {
        "id": "XAnkoop2Mvn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Định nghĩa mô hình"
      ],
      "metadata": {
        "id": "96jGtYjBPBS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Định nghĩa hàm focal loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'): #Cho kết quả tổng quát trên toàn bộ batch\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)  # pt là xác suất của nhãn đúng\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "\n",
        "# Mô hình DNN\n",
        "class BugLocalization(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BugLocalization, self).__init__()\n",
        "        # Các lớp ẩn với kích thước 300 và 150 node\n",
        "        self.fc1 = nn.Linear(input_dim, 300)\n",
        "        self.fc2 = nn.Linear(300, 150)\n",
        "        self.fc3 = nn.Linear(150, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "tPaVtgshPEJh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huấn luyện mô hình"
      ],
      "metadata": {
        "id": "BiIkf7YOPFKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "2dxWl_0nMyqx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, mini_batches, criterion, optimizer, device, project_name, epochs):\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    save_dir = f\"{models_dir}/{project_name}_training_model\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "        # 1 mini_batch: 1 list các batch\n",
        "        # 1 batch: (batch_inputs, batch_labels)\n",
        "        for batch_inputs, batch_labels in mini_batches:\n",
        "            batch_inputs = torch.tensor(batch_inputs, dtype=torch.float32).to(device)\n",
        "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs)\n",
        "            loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            #In ra mỗi 50 batch\n",
        "            if batch_count % 50 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{epochs}], Batch [{batch_count}/{len(mini_batches)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(mini_batches)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pt\")\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss,\n",
        "        }\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Model checkpoint saved at {save_path}\")"
      ],
      "metadata": {
        "id": "84saq-nxTdCD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metrics"
      ],
      "metadata": {
        "id": "l_ORaPDhVnGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(predictions, labels, k_values=[1, 5, 10, 15]):\n",
        "    \"\"\"\n",
        "    Tính độ chính xác Top-k.\n",
        "\n",
        "    Với mỗi giá trị k, kiểm tra xem nhãn đúng có nằm trong top-k dự đoán hay không.\n",
        "\n",
        "    Tham số:\n",
        "        predictions (Tensor): Tensor có shape (batch_size, num_classes), chứa điểm số dự đoán.\n",
        "        labels (Tensor): Tensor có shape (batch_size,), chứa chỉ số của lớp đúng.\n",
        "        k_values (List[int]): Danh sách các giá trị k để tính Top-k Accuracy.\n",
        "\n",
        "    Trả về:\n",
        "        dict: Dictionary chứa Top-k Accuracy cho từng giá trị k.\n",
        "    \"\"\"\n",
        "    top_k_accuracies = {}\n",
        "\n",
        "    # Kiểm tra kích thước của predictions và thay đổi dim nếu cần\n",
        "    if predictions.dim() == 1 and predictions.size(0) == 1:\n",
        "        predictions = predictions.unsqueeze(0)  # Thêm một chiều cho predictions nếu nó chỉ có một phần tử\n",
        "    num_classes = predictions.shape[1]\n",
        "\n",
        "    for k in k_values:\n",
        "        actual_k = min(k, num_classes)\n",
        "        # Lấy top-k chỉ số dự đoán\n",
        "        _, top_k_indices = torch.topk(predictions, actual_k, largest=True, sorted=False)\n",
        "\n",
        "        # Kiểm tra xem nhãn đúng có nằm trong top-k không\n",
        "        top_k_correct = torch.sum((top_k_indices == labels.view(-1, 1)).any(dim=1))\n",
        "        top_k_accuracy = top_k_correct.item() / len(labels)\n",
        "\n",
        "        top_k_accuracies[k] = top_k_accuracy\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "\n",
        "def mean_reciprocal_rank(predictions, labels):\n",
        "    \"\"\"\n",
        "    Tính Mean Reciprocal Rank (MRR).\n",
        "\n",
        "    Với mỗi mẫu, tìm vị trí (rank) của nhãn đúng trong danh sách dự đoán và lấy nghịch đảo (1/rank).\n",
        "\n",
        "    Tham số:\n",
        "        predictions (Tensor): Tensor có shape (batch_size, num_classes), chứa điểm số dự đoán.\n",
        "        labels (Tensor): Tensor có shape (batch_size,), chứa chỉ số của lớp đúng.\n",
        "\n",
        "    Trả về:\n",
        "        float: Giá trị Mean Reciprocal Rank.\n",
        "    \"\"\"\n",
        "\n",
        "    if predictions.dim() == 1 and predictions.size(0) == 1:\n",
        "        predictions = predictions.unsqueeze(0)  # Thêm một chiều cho predictions nếu nó chỉ có một phần tử\n",
        "\n",
        "    # Lấy top indices (chỉ số các lớp dự đoán từ cao xuống thấp)\n",
        "    _, top_indices = torch.topk(predictions, predictions.shape[1], largest=True, sorted=False)\n",
        "\n",
        "    mrr = 0.0\n",
        "\n",
        "    # Duyệt qua từng mẫu trong batch\n",
        "    for i in range(len(labels)):\n",
        "        correct_idx = labels[i].item()  # Lấy chỉ số lớp đúng của mẫu i\n",
        "\n",
        "        # Tìm vị trí (thứ hạng) của lớp đúng trong top_indices\n",
        "        rank = (top_indices[i] == correct_idx).nonzero(as_tuple=True)\n",
        "\n",
        "        if len(rank[0]) > 0:\n",
        "            rank = rank[0].item() + 1  # Thứ hạng bắt đầu từ 1\n",
        "            mrr += 1.0 / rank\n",
        "        else:\n",
        "            mrr += 0.0  # Nếu lớp đúng không có trong top indices, MRR cho mẫu này là 0\n",
        "\n",
        "    mrr = mrr / len(labels)  # Tính trung bình MRR cho cả batch\n",
        "    return mrr\n",
        "\n",
        "\n",
        "def mean_average_precision(predictions, labels, k_values=[1, 5, 10, 15]):\n",
        "    \"\"\"\n",
        "    Tính Mean Average Precision (MAP) tại nhiều mức top-k.\n",
        "\n",
        "    Với mỗi k, tính trung bình Precision tại vị trí chứa nhãn đúng (nếu có) trong top-k dự đoán.\n",
        "\n",
        "    Tham số:\n",
        "        predictions (Tensor): Tensor có shape (batch_size, num_classes), chứa điểm số dự đoán.\n",
        "        labels (Tensor): Tensor có shape (batch_size,), chứa chỉ số của lớp đúng.\n",
        "        k_values (List[int]): Danh sách các giá trị k để tính MAP@k.\n",
        "\n",
        "    Trả về:\n",
        "        float: Giá trị trung bình của MAP@k trên tất cả các giá trị k.\n",
        "    \"\"\"\n",
        "    # Nếu predictions chỉ có một chiều, thêm một chiều để có dạng (batch_size, num_classes)\n",
        "    if predictions.dim() == 1:\n",
        "        predictions = predictions.unsqueeze(0)  # Thêm một chiều cho predictions nếu chỉ có một phần tử\n",
        "\n",
        "    map_score = 0.0\n",
        "\n",
        "    # Giới hạn k không vượt quá số lớp trong predictions\n",
        "    max_k = min(max(k_values), predictions.size(1))\n",
        "\n",
        "    _, top_k_indices = torch.topk(predictions, max_k, largest=True, sorted=False)\n",
        "\n",
        "    for k in k_values:\n",
        "        avg_precision = 0.0\n",
        "        for i in range(len(labels)):\n",
        "            relevant_files = (top_k_indices[i, :k] == labels[i]).sum().item()\n",
        "            precision_at_k = relevant_files / k\n",
        "            avg_precision += precision_at_k\n",
        "\n",
        "        map_score += avg_precision / len(labels)\n",
        "\n",
        "    map_score = map_score / len(k_values)\n",
        "    return map_score"
      ],
      "metadata": {
        "id": "p7z0XFFTVrB7"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Đánh giá mô hình"
      ],
      "metadata": {
        "id": "EO8lCX5k_xGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_test, criterion, device, k_values=[1, 5, 10, 15]):\n",
        "  model.eval()\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  total_loss = 0.0\n",
        "  total_samples = 0\n",
        "  top_k_accuracies = {k: 0 for k in k_values}\n",
        "  mrr_score = 0.0\n",
        "  map_score = 0.0\n",
        "\n",
        "  all_preds =[]\n",
        "  all_labels = []\n",
        "  batch_results = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_test, y_test in data_test:\n",
        "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "      if y_test.dim() == 0:\n",
        "            y_test = y_test.unsqueeze(0)  # Thêm một chiều để có dạng (1,)\n",
        "\n",
        "      outputs = model(X_test)\n",
        "\n",
        "      # Đảm bảo outputs có cùng kích thước với y_test\n",
        "      #print(f\"outputs.shape: {outputs.shape}, y_test.shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "      loss = criterion(outputs, y_test)\n",
        "      total_loss += loss.item() * y_test.size(0)\n",
        "      total_samples += y_test.size(0)\n",
        "\n",
        "\n",
        "      top_k_acc = top_k_accuracy(outputs, y_test)\n",
        "      for k in k_values:\n",
        "        top_k_accuracies[k] += top_k_acc[k]  * y_test.size(0)\n",
        "\n",
        "      mrr_score += mean_reciprocal_rank(outputs, y_test) * y_test.size(0)\n",
        "      map_score += mean_average_precision(outputs, y_test) * y_test.size(0)\n",
        "\n",
        "      all_preds.append(outputs)\n",
        "      all_labels.append(y_test)\n",
        "\n",
        "    # Gộp thành tensor duy nhất\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Trung bình các chỉ số cho tất cả các batch\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_top_k_accuracy = {k: top_k_accuracies[k] / total_samples for k in k_values}\n",
        "    avg_mrr = mrr_score / total_samples\n",
        "    avg_map = map_score / total_samples\n",
        "\n",
        "        # Kết quả tổng hợp\n",
        "    results = {\n",
        "            'loss': avg_loss,\n",
        "            'top_k_accuracy': avg_top_k_accuracy,\n",
        "            'mrr': avg_mrr,\n",
        "            'map': avg_map\n",
        "    }\n",
        "    return results"
      ],
      "metadata": {
        "id": "AgVyb7n4VuXd"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BugLocalization(input_dim=5)\n",
        "criterion = FocalLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zkweXOt2YwO7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lấy 1 phần để thử pipeline"
      ],
      "metadata": {
        "id": "cgeJD2n5_oAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_train), len(data_test)) # Fold 0 train, fold 1 test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi4LwWN4o7SH",
        "outputId": "5ec28cc8-0f81-4ba9-a0e3-5b1e1c87b034"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99523 929808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lấy 1 mẫu nhỏ của fold 0 để train và test thử pipeline\n",
        "data_train_small = stratified(data_train, 1000)\n",
        "data_test_small = stratified(data_test, 9000)\n",
        "\n",
        "data_train_small = create_data(data_train_small, glove_model)\n",
        "data_test_small = create_data(data_test_small, glove_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-toeioqXu1Ou",
        "outputId": "fbec2c58-6e0f-4f25-b6bb-d56ca652d7fe"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data for 1000 samples\n",
            "Creating data for 9000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batches = create_minibatch(data_train_small, batch_size)\n",
        "\n",
        "#Training\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TRAINING\")\n",
        "train_model(model, mini_batches, criterion, optimizer, device, 'temp', epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yYDhnn0qLPW",
        "outputId": "bcba7161-9ca9-4ae8-e9bc-907c0521444d"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating minibatch for 1000 samples\n",
            "---------------------------------------------\n",
            "\n",
            "TRAINING\n",
            "Epoch [1/30], Loss: 0.0306\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_1.pt\n",
            "Epoch [2/30], Loss: 0.0295\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_2.pt\n",
            "Epoch [3/30], Loss: 0.0289\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_3.pt\n",
            "Epoch [4/30], Loss: 0.0288\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_4.pt\n",
            "Epoch [5/30], Loss: 0.0297\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_5.pt\n",
            "Epoch [6/30], Loss: 0.0283\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_6.pt\n",
            "Epoch [7/30], Loss: 0.0289\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_7.pt\n",
            "Epoch [8/30], Loss: 0.0287\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_8.pt\n",
            "Epoch [9/30], Loss: 0.0289\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_9.pt\n",
            "Epoch [10/30], Loss: 0.0281\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_10.pt\n",
            "Epoch [11/30], Loss: 0.0292\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_11.pt\n",
            "Epoch [12/30], Loss: 0.0289\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_12.pt\n",
            "Epoch [13/30], Loss: 0.0299\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_13.pt\n",
            "Epoch [14/30], Loss: 0.0314\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_14.pt\n",
            "Epoch [15/30], Loss: 0.0293\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_15.pt\n",
            "Epoch [16/30], Loss: 0.0304\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_16.pt\n",
            "Epoch [17/30], Loss: 0.0294\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_17.pt\n",
            "Epoch [18/30], Loss: 0.0290\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_18.pt\n",
            "Epoch [19/30], Loss: 0.0290\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_19.pt\n",
            "Epoch [20/30], Loss: 0.0275\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_20.pt\n",
            "Epoch [21/30], Loss: 0.0283\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_21.pt\n",
            "Epoch [22/30], Loss: 0.0299\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_22.pt\n",
            "Epoch [23/30], Loss: 0.0289\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_23.pt\n",
            "Epoch [24/30], Loss: 0.0278\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_24.pt\n",
            "Epoch [25/30], Loss: 0.0293\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_25.pt\n",
            "Epoch [26/30], Loss: 0.0271\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_26.pt\n",
            "Epoch [27/30], Loss: 0.0273\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_27.pt\n",
            "Epoch [28/30], Loss: 0.0283\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_28.pt\n",
            "Epoch [29/30], Loss: 0.0283\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_29.pt\n",
            "Epoch [30/30], Loss: 0.0299\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/temp_training_model/epoch_30.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TESTING\")\n",
        "results = evaluate_model(model, data_test_small, criterion, device) #Lỗi hàm top -k, evaluate_model --> in ra key nào cx giống nhau --> check lại evaluation metrics và evaluate_model\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaehEuS0CIR",
        "outputId": "ebbbadb5-470c-4b8c-fa33-a03edf26f02c"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "\n",
            "TESTING\n",
            "{'loss': 0.02041959121880225, 'top_k_accuracy': {1: 0.9997777777777778, 5: 0.9997777777777778, 10: 0.9997777777777778, 15: 0.9997777777777778}, 'mrr': 0.9997777777777778, 'map': 0.3415907407407639}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation cho project"
      ],
      "metadata": {
        "id": "KzCAhTGy_jhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(project_name, k_fold, model, criterion, optimizer, device, batch_size, epochs):\n",
        "  for i in range(k_fold - 1):\n",
        "    data_train, data_test = load_fold_data(project_name, i)\n",
        "    data_train = stratified(data_train, 50000)\n",
        "    #data_test = stratified(data_test, 10000)\n",
        "    # Tạo data sau khi đã xử lý features\n",
        "    data_train = create_data(data_train, glove_model) #data_train [(vector, label),....]\n",
        "    data_test = create_data(data_test, glove_model) #[(vector, label),....]\n",
        "    mini_batches = create_minibatch(data_train, batch_size) # 1 list các batch\n",
        "\n",
        "    #Training\n",
        "    print(\"---------------------------------------------\\n\")\n",
        "    print(f\"TRAINING ON FOLD {i}\")\n",
        "    train_model(model, mini_batches, criterion, optimizer, device, project_name, epochs)\n",
        "    #Testing\n",
        "    print(\"---------------------------------------------\\n\")\n",
        "    print(f\"TESTING ON FOLD {i+1}\")\n",
        "    results = evaluate_model(model, data_test, criterion, device)\n",
        "    print(f\"Fold{i} -: {results}\")"
      ],
      "metadata": {
        "id": "-hto2ySEM2IR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_validation('aspectj', 3, model, criterion, optimizer, device, batch_size, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ewwWKyjqb0H1",
        "outputId": "dc9572d0-a911-4038-f1bc-4120f3532aea"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data for 50000 samples\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-19adc31e814c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aspectj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-d428dd6d5ba1>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(project_name, k_fold, model, criterion, optimizer, device, batch_size, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Tạo data sau khi đã xử lý features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data_train [(vector, label),....]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[(vector, label),....]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 list các batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-0d555ab6e073>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(dataset, glove_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#pair: (bug_report, src_file, label) --> label: pair[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-167a0d364774>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(pair, dataset, glove_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlex_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lexical_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msem_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprev_bug_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_bugs_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchange_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_code_change_history_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mchange_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bug_fixing_freq_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-7e8743852bc1>\u001b[0m in \u001b[0;36mget_similar_bugs_score\u001b[0;34m(pair, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_similar_bugs_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#pair (BugReport, SrcFile, Label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprevious_bug_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_previous_bug_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_bug_reports\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-a5c894eb956e>\u001b[0m in \u001b[0;36mget_previous_bug_reports\u001b[0;34m(bug_report, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_previous_bug_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msource_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_source_files_for_bug_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mbug_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_bug_reports_fix_source_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbug_reports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b0e646fcb614>\u001b[0m in \u001b[0;36mfind_source_files_for_bug_report\u001b[0;34m(target_bug_report, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_source_files_for_bug_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_bug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tìm tất cả các SourceFile liên quan đến BugReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msource_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource_file\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbug_report\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_bug_report\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msource_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b0e646fcb614>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_source_files_for_bug_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_bug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tìm tất cả các SourceFile liên quan đến BugReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msource_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource_file\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbug_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbug_report\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_bug_report\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msource_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}