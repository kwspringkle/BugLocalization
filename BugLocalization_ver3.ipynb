{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDlOi0xvMHh"
      },
      "source": [
        "# **Bug Localization - DNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cfBzQkcvN_N",
        "outputId": "a05cd724-cb19-4202-b4a3-4da3132b8f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpqI0hXevPNO"
      },
      "outputs": [],
      "source": [
        "project_path = 'drive/MyDrive/Colab Notebooks/BugLocalization'\n",
        "\n",
        "import sys\n",
        "sys.path.append(project_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGwNUm-8vyfh"
      },
      "source": [
        "## 1. Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avsWi_m6vx_b",
        "outputId": "f0c6157a-d1a2-4c76-dcdf-ff29c937b39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting inflection\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: inflection\n",
            "Successfully installed inflection-0.5.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting javalang\n",
            "  Downloading javalang-0.13.0-py3-none-any.whl.metadata (805 bytes)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from javalang) (1.17.0)\n",
            "Downloading javalang-0.13.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: javalang\n",
            "Successfully installed javalang-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pickle\n",
        "!pip install inflection\n",
        "!pip install nltk\n",
        "!pip install javalang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxkloOn5v1Dv",
        "outputId": "99bac4e4-5daa-4b62-9b91-2635e801d921"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from preprocessing import Parser, ReportPreprocessing, SrcPreprocessing\n",
        "from projectDatasets import aspectj, eclipse, swt, tomcat, birt\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHruwQegv4MT",
        "outputId": "2c614890-2cb8-4680-c9cf-352d384b4036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data load successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Đường dẫn thư mục models trong dự án\n",
        "models_dir = f\"{project_path}/models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "bug_reports_path = f\"{models_dir}/preprocessed_bug_reports.pkl\"\n",
        "src_reports_path = f\"{models_dir}/preprocessed_src_reports.pkl\"\n",
        "\n",
        "with open(src_reports_path, \"rb\") as f:\n",
        "    aspectj_src_files = pickle.load(f)\n",
        "    eclipse_src_files = pickle.load(f)\n",
        "    swt_src_files = pickle.load(f)\n",
        "    tomcat_src_files = pickle.load(f)\n",
        "    birt_src_files = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(bug_reports_path, \"rb\") as f:\n",
        "    aspectj_bug_reports = pickle.load(f)\n",
        "    eclipse_bug_reports = pickle.load(f)\n",
        "    swt_bug_reports = pickle.load(f)\n",
        "    tomcat_bug_reports = pickle.load(f)\n",
        "    birt_bug_reports = pickle.load(f)\n",
        "\n",
        "print(\"Data load successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIMVaAXHv_mv"
      },
      "source": [
        "## 2. Chia fold, gán nhãn dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZr0ak66Rzjn"
      },
      "source": [
        "### Chia fold cho bug reports\n",
        "- 3 fold cho Aspectj\n",
        "- 10 fold cho các projects còn lại\n",
        "- Sắp xếp bug reports rồi chia theo thời gian từ cũ đến mới (fold i để test và fold i + 1 để train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM-L7BjbRzPd"
      },
      "outputs": [],
      "source": [
        "# Sắp xếp lại bug reports theo thời gian từ cũ đến mới\n",
        "sorted_aspectj_bug_reports = sorted(aspectj_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_eclipse_bug_reports = sorted(eclipse_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_swt_bug_reports = sorted(swt_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_tomcat_bug_reports = sorted(tomcat_bug_reports.items(), key=lambda x: x[1].report_time)\n",
        "sorted_birt_bug_reports = sorted(birt_bug_reports.items(), key=lambda x: x[1].report_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZTGrYzHSF7e",
        "outputId": "4738842b-2f7c-4414-d8ca-64f4fdff56cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2002-03-13 12:40:54\n",
            "2002-12-30 16:40:03\n",
            "593\n"
          ]
        }
      ],
      "source": [
        "# Lấy một báo cáo lỗi cụ thể, ví dụ báo cáo lỗi đầu tiên trong danh sách\n",
        "bug_report = sorted_aspectj_bug_reports[0]\n",
        "bug_report2 = sorted_aspectj_bug_reports[1]\n",
        "\n",
        "# Truy cập vào report_time của bug report\n",
        "report_time = bug_report[1].report_time  # bug_report[1] là đối tượng BugReport\n",
        "report_time2 = bug_report2[1].report_time\n",
        "\n",
        "# In ra thời gian báo cáo, kiểm tra xem đúng thứ tự từ cũ đến mới chưa\n",
        "print(report_time)\n",
        "print(report_time2)\n",
        "\n",
        "print(len(sorted_aspectj_bug_reports))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RU9lYcSPvh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Chia fold theo thứ tự thời gian\n",
        "def split_time_ordered_folds(sorted_data, n_folds):\n",
        "    fold_size = len(sorted_data) // n_folds # Tính xấp xỉ fold_size cần\n",
        "    folds = []\n",
        "\n",
        "    # Tạo các indices cho từng fold\n",
        "    indices = np.arange(len(sorted_data)) # Tạo mảng chỉ số cho các phần tử reports\n",
        "    fold_indices = np.array_split(indices, n_folds) # Chia mảng indices thành n folds, kích thước bằng nhau nhất có thể\n",
        "\n",
        "    # Lưu các fold và indices của chúng\n",
        "    fold_data = []\n",
        "    for fold_idx in fold_indices:\n",
        "        fold_data.append([sorted_data[i] for i in fold_idx])\n",
        "        # Thêm data vào fold data nếu chỉ số có ở trong tập fold_indices\n",
        "\n",
        "    return fold_data\n",
        "\n",
        "# Chia thành các fold theo yêu cầu\n",
        "\n",
        "# AspectJ chia 3 fold\n",
        "aspectj_folds = split_time_ordered_folds(sorted_aspectj_bug_reports, 3)\n",
        " # Các project khác chia 10 fold\n",
        "eclipse_folds = split_time_ordered_folds(sorted_eclipse_bug_reports, 10)\n",
        "swt_folds = split_time_ordered_folds(sorted_swt_bug_reports, 10)\n",
        "tomcat_folds = split_time_ordered_folds(sorted_tomcat_bug_reports, 10)\n",
        "birt_folds = split_time_ordered_folds(sorted_birt_bug_reports, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP6GbdW1WPMw"
      },
      "source": [
        "### Gán nhãn dữ liệu\n",
        "- Nếu liên quan --> label là 1\n",
        "- Nếu không liên quan --> label là 0\n",
        "- Vấn đề: label 0 sẽ có rất nhiều, trong khi label 1 rất ít (skewed distribution --> cách giải quyết: data-sample manipulation, cost-sensitive learning(focal-loss function))\n",
        "  - Giải quyết bài toán: Chọn ra top k các lớp 0 có similarity lớn, khó phân biệt với lớp 1 để train(hard negative sampling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVCm8iyZbmkB"
      },
      "source": [
        "Hàm tính cosine similarity giữa 2 văn bản (lexical similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqCrXsGvb7MI"
      },
      "outputs": [],
      "source": [
        "def bug_report_to_text(bug_report):\n",
        "    text_parts = []\n",
        "\n",
        "    # Kiểm tra và thêm phần tóm tắt đã gắn nhãn từ bug_report\n",
        "    if bug_report.pos_tagged_summary is not None and 'unstemmed' in bug_report.pos_tagged_summary:\n",
        "        text_parts.append(\" \".join(bug_report.pos_tagged_summary['unstemmed']))\n",
        "\n",
        "    # Kiểm tra và thêm phần mô tả đã gắn nhãn từ bug_report\n",
        "    if bug_report.pos_tagged_description is not None and 'unstemmed' in bug_report.pos_tagged_description:\n",
        "        text_parts.append(\" \".join(bug_report.pos_tagged_description['unstemmed']))\n",
        "\n",
        "    # Kết hợp tất cả các phần và trả về kết quả\n",
        "    text = \" \".join(text_parts)\n",
        "    return text\n",
        "\n",
        "def src_report_to_text(src_report):\n",
        "    text_parts = []\n",
        "\n",
        "    # Kiểm tra và thêm phần bình luận đã gắn nhãn từ source_report\n",
        "    if src_report.pos_tagged_comments is not None and 'unstemmed' in src_report.pos_tagged_comments:\n",
        "        text_parts.append(\" \".join(src_report.pos_tagged_comments['unstemmed']))\n",
        "\n",
        "    # Kiểm tra và thêm tên lớp từ source_report\n",
        "    if src_report.class_names is not None:\n",
        "        text_parts.append(\" \".join(src_report.class_names))\n",
        "\n",
        "    # Kiểm tra và thêm các thuộc tính từ source_report\n",
        "    if src_report.attributes is not None:\n",
        "        text_parts.append(\" \".join(src_report.attributes))\n",
        "\n",
        "    # Kiểm tra và thêm tên phương thức từ source_report\n",
        "    if src_report.method_names is not None:\n",
        "        text_parts.append(\" \".join(src_report.method_names))\n",
        "\n",
        "    # Kiểm tra và thêm biến từ source_report\n",
        "    if src_report.variables is not None:\n",
        "        text_parts.append(\" \".join(src_report.variables))\n",
        "\n",
        "    # Kết hợp tất cả các phần và trả về kết quả\n",
        "    text = \" \".join(text_parts)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC0rTct3b0jm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Đầu vào: 2 text (bug report, src_file)\n",
        "def lexical_similarity(bug_report, src_report):\n",
        "    # Chuyển bug_report và src_report thành văn bản\n",
        "    bug_report_text = bug_report_to_text(bug_report)\n",
        "    src_report_text = src_report_to_text(src_report)\n",
        "\n",
        "    # Kết hợp văn bản của bug_report và src_report\n",
        "    all_texts = [bug_report_text, src_report_text]\n",
        "\n",
        "    # Tính TF-IDF cho văn bản\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Lấy các vector TF-IDF cho bug_report và src_report\n",
        "    bug_report_tfidf = tfidf_matrix[0]  # Vector của bug_report\n",
        "    src_report_tfidf = tfidf_matrix[1]  # Vector của src_report\n",
        "\n",
        "    # Tính cosine similarity giữa bug_report và src_report\n",
        "    similarity = cosine_similarity(bug_report_tfidf, src_report_tfidf)\n",
        "\n",
        "    return similarity[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2xM38-ecQ4J"
      },
      "source": [
        "Lấy ra tập dương tính"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fA9m7AlcTrM"
      },
      "outputs": [],
      "source": [
        "# Đầu vào: 1 bug report, tập source files của project đó\n",
        "# Đầu ra: các source files liên quan đến bug report đó\n",
        "\n",
        "def src_positive(bug_report, src_files):\n",
        "  src_pos = []\n",
        "  bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        ")\n",
        "  for src_address, src_file in src_files.items():\n",
        "    if src_address in bug_report_fixed_files:\n",
        "      src_pos.append(src_file)\n",
        "  return src_pos #[src_file, src_file,....]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wOZV2HAdD-L"
      },
      "source": [
        "Lấy ra tập âm tính\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2LwlE3QdF83"
      },
      "outputs": [],
      "source": [
        "# Đầu vào: 1 bug report, tập source files của project đó\n",
        "# Đầu ra: top 50 file không liên quan đến bug report (top 50 negative cho mỗi bug reports)\n",
        "def src_negative(bug_report, src_files, k=50):\n",
        "  src_neg = []\n",
        "  bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        "  )\n",
        "  sim_scores = []\n",
        "  for src_address , src_file in src_files.items():\n",
        "    if src_address not in bug_report_fixed_files: # Kiểm tra không ở trong tập positive\n",
        "      sim_score = lexical_similarity(bug_report, src_file) # Tính cosine similarity giữa 2 file\n",
        "      sim_scores.append((src_file, sim_score))\n",
        "  sim_scores.sort(key=lambda x: x[1], reverse=True) # Sắp xếp theo phần tử thứ 2 (sim_score), reverse=True --> giảm dần theo sim_score\n",
        "\n",
        "  for i in range(k):\n",
        "    src_neg.append(sim_scores[i][0])\n",
        "  return src_neg # [src_file, src_file,...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgD26wZsfGAN"
      },
      "source": [
        "Label data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyeg2MgClopR",
        "outputId": "aafdff69-9ac6-44f6-ada3-7dcaacc72af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tests/src/org/aspectj/systemtest/ajc150/Ajc150Tests.java', ' weaver/src/org/aspectj/weaver/reflect/ReflectionBasedReferenceTypeDelegateFactory.java', '.']\n"
          ]
        }
      ],
      "source": [
        "print(aspectj_folds[0][0][1].fixed_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82hkxB8-fJW4"
      },
      "outputs": [],
      "source": [
        "# Đầu vào: 1 fold bug_reports dùng để train, src_files của project đó\n",
        "# Đầu ra: 1 list các tuple (bug_report, src_file, label)\n",
        "\n",
        "# 1 fold bug_reports có dạng [(bug_id, BugReport Object),....]\n",
        "\n",
        "def label_data_train(prj_bug_reports_fold, src_files):\n",
        "  data_train = []\n",
        "  for bug_id, bug_report in prj_bug_reports_fold:\n",
        "    src_pos = src_positive(bug_report, src_files)\n",
        "    src_neg = src_negative(bug_report, src_files)\n",
        "    for src_file in src_pos:\n",
        "      data_train.append((bug_report, src_file, 1))\n",
        "    for src_file in src_neg:\n",
        "      data_train.append((bug_report, src_file, 0))\n",
        "  return data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfwJFOOghhRe"
      },
      "outputs": [],
      "source": [
        "# Đầu vào: 1 fold bug_reports dùng để test, src_files của project đó\n",
        "# Đầu ra: 1 list các tuple (bug_report, src_file, label)\n",
        "\n",
        "def label_data_test(prj_bug_reports_fold, src_files):\n",
        "  data_test = []\n",
        "  for bug_id, bug_report in prj_bug_reports_fold:\n",
        "    bug_report_fixed_files = list(\n",
        "    f.strip() for f in bug_report.fixed_files if f.strip() != '.' and f.strip() != ''\n",
        "    )\n",
        "    for src_address, src_file in src_files.items():\n",
        "      if src_address in bug_report_fixed_files:\n",
        "        data_test.append((bug_report, src_file, 1))\n",
        "      else:\n",
        "        data_test.append((bug_report, src_file, 0))\n",
        "  return data_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kApTXU47yU4J"
      },
      "source": [
        "### Label và lưu lại dữ liệu train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glY2_3ZE0AhK"
      },
      "outputs": [],
      "source": [
        "def save_fold_data(project_name, project_src_files, project_folds):\n",
        "    project_dir = f\"{models_dir}/{project_name}\"\n",
        "    print(f\"Saving project: {project_name}\")\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "    n_folds = len(project_folds)\n",
        "\n",
        "    for i in range(n_folds - 1):  # fold i dùng để train, fold i+1 để test\n",
        "        # Gán dữ liệu train/test\n",
        "        data_train= label_data_train(project_folds[i], project_src_files) #Train fold i\n",
        "        data_test = label_data_test(project_folds[i + 1], project_src_files) # Test fold i + 1\n",
        "\n",
        "        # Lưu bằng pickle\n",
        "        with open(os.path.join(project_dir, f\"fold_{i}_train.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(data_train, f)\n",
        "\n",
        "        with open(os.path.join(project_dir, f\"fold_{i}_test.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(data_test, f)\n",
        "\n",
        "        print(f\"Saved fold {i}: train={len(data_train)}, test={len(data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3uYlscFSk81",
        "outputId": "e4b27f1a-8dfc-422a-e7a1-d0b78b31bf15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving project: aspectj\n",
            "Saved fold 0: train=10423, test=929808\n",
            "Saved fold 1: train=10168, test=925112\n"
          ]
        }
      ],
      "source": [
        "save_fold_data ('aspectj', aspectj_src_files, aspectj_folds)\n",
        "#save_fold_data ('eclipse', eclipse_src_files, eclipse_folds)\n",
        "#save_fold_data ('swt', swt_src_files, swt_folds)\n",
        "#save_fold_data ('tomcat', tomcat_src_files, tomcat_folds)\n",
        "#save_fold_data('birt', birt_src_files, birt_folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA8dG0_kTPjN"
      },
      "outputs": [],
      "source": [
        "def load_fold_data(project_name, fold_idx):\n",
        "    project_dir = f\"{models_dir}/{project_name}\"\n",
        "\n",
        "    train_file = os.path.join(project_dir, f\"fold_{fold_idx}_train.pkl\")\n",
        "    test_file = os.path.join(project_dir, f\"fold_{fold_idx}_test.pkl\")\n",
        "\n",
        "    # Load tập train, tập test\n",
        "    with open(train_file, \"rb\") as f:\n",
        "        data_train = pickle.load(f)\n",
        "    with open(test_file, \"rb\") as f:\n",
        "        data_test = pickle.load(f)\n",
        "    return data_train, data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z0mNXxOSugF",
        "outputId": "9ab9cfb9-a687-46a3-dc18-b6f490e854cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 10423, Test data size: 929808\n"
          ]
        }
      ],
      "source": [
        "# Load fold 0 của project aspectj để kiểm tra\n",
        "# Tạm thời đang lấy fold để test là fold 1 để train(do test size quá lớn)\n",
        "data_train, data_test  = load_fold_data('aspectj', 0)\n",
        "print(f\"Train data size: {len(data_train)}, Test data size: {len(data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jPodvNmhEsT",
        "outputId": "1bb9256a-0774-4a42-f785-1830b15a5c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label 1: 268, Label 0: 9900\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "count_label_1 = 0\n",
        "for report, file, label in data_test:\n",
        "  count += 1\n",
        "  if label == 1:\n",
        "    count_label_1 += 1\n",
        "    #print(f\"{report}, {file}, {label}\")\n",
        "print(f\"Label 1: {count_label_1}, Label 0: {count - count_label_1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wFg2zJHvNMQ"
      },
      "source": [
        "## 3. Trích xuất đặc trưng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABy1gaQwvQVD"
      },
      "source": [
        "### 3.1. Lexical similarity\n",
        "\n",
        "- Hàm bug2text và src2text ở trên"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT3UhEgovwl-"
      },
      "outputs": [],
      "source": [
        "# Đầu vào: 1 pair trong tập data train (1 tuple (bug_report, source_file, label))\n",
        "# Đầu ra: lexical similarity của pair đó\n",
        "\n",
        "def get_lexical_similarity(pair):\n",
        "    similarity = lexical_similarity(pair[0], pair[1])\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUL6tAvknOX9",
        "outputId": "ea0d9b80-31a5-4e60-c753-5b4b88605e88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.24505498492568153)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_lexical_similarity(data_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xrpJ8u9wK9i"
      },
      "source": [
        "### 3.2. Semantic similarity\n",
        "Do có lexical mismatch khi bug reports sử dụng ngôn ngữ tự nhiên, còn source code lại sử dụng ngôn ngữ lập trình --> Tìm hiểu thêm về semantic relationship giữa các từ phụ thuộc vào context nó xuất hiện\n",
        "\n",
        "- Sử dụng pretrained Glove để chuyển từ --> vectors\n",
        "  - Glove: word embeddings, dựa trên co-occurrence matrix của các từ trong văn bản. Glove sẽ tìm 1 bộ các vector sao cho tần suất đồng xuất hiện của các từ trong không gian vector gần với tần suất đồng xuất hiện trong co-occurence matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA_Me6PowNmm",
        "outputId": "fa07dd9b-152e-4879-dce7-f71c036574c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe model...\n",
            "Model loaded with 400000 words.\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained glove\n",
        "glove_path = '/content/drive/MyDrive/Colab Notebooks/BugLocalization/glove.6B/glove.6B.100d.txt'\n",
        "\n",
        "def load_glove_model(glove_file):\n",
        "    print(\"Loading GloVe model...\")\n",
        "    model = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            word = parts[0]\n",
        "            vector = np.asarray(parts[1:], dtype='float32')\n",
        "            model[word] = vector\n",
        "    print(f\"Model loaded with {len(model)} words.\")\n",
        "    return model\n",
        "\n",
        "glove_model = load_glove_model(glove_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb2cDWgjwOlu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def embed_text_tensor(text, glove_dict, tfidf_weights, vocab, dim=100):\n",
        "    tokens = text.split()\n",
        "    vecs = []\n",
        "    weights = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in glove_dict and token in vocab:\n",
        "            vecs.append(torch.tensor(glove_dict[token]))\n",
        "            weights.append(tfidf_weights.get(token, 0))  # Lấy TF-IDF weight nếu có, nếu không thì 0\n",
        "\n",
        "    if not vecs:\n",
        "        return torch.zeros(dim)\n",
        "\n",
        "    vecs = torch.stack(vecs)  # (n_tokens, dim)\n",
        "    weights = torch.tensor(weights).unsqueeze(1)  # (n_tokens, 1)\n",
        "\n",
        "    # Nhân từng vector với trọng số TF-IDF\n",
        "    weighted_vecs = vecs * weights\n",
        "\n",
        "    # Tính tổng và chuẩn hóa bằng cách chia cho tổng trọng số\n",
        "    return weighted_vecs.sum(dim=0) / weights.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVOxpwBKwPnF"
      },
      "outputs": [],
      "source": [
        "def get_semantic_similarity(pair,glove_dict, dim=100, device=\"cpu\"):\n",
        "    # Chuyển bug_report và src_report thành văn bản\n",
        "    bug_report = pair[0]\n",
        "    src_report= pair[1]\n",
        "    bug_report_text = bug_report_to_text(bug_report)  # 1 bug report\n",
        "    src_report_text = src_report_to_text(src_report)  # 1 source report\n",
        "\n",
        "    # Fit TF-IDF để lấy idf weight chỉ cho cặp bug_report và src_report\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf.fit([bug_report_text, src_report_text])\n",
        "    vocab = tfidf.vocabulary_\n",
        "    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
        "\n",
        "    # Embed text thành tensor\n",
        "    def embed_text_tensor_with_tfidf(text):\n",
        "        return embed_text_tensor(text, glove_dict, idf_weights, vocab, dim)\n",
        "\n",
        "    # Embed bug_report và src_report thành vectors\n",
        "    bug_vec = embed_text_tensor_with_tfidf(bug_report_text).to(device)\n",
        "    src_vec = embed_text_tensor_with_tfidf(src_report_text).to(device)\n",
        "\n",
        "    # Tính cosine similarity giữa bug_report và src_report\n",
        "    similarity = cosine_similarity([bug_vec.cpu().numpy()], [src_vec.cpu().numpy()])\n",
        "\n",
        "    return similarity[0][0]  # Trả về giá trị similarity duy nhất"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1jxthwsnKrj",
        "outputId": "f3fa4c97-4759-460b-de82-fcaad3aec719"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.860221418588926)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_semantic_similarity(data_train[0], glove_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omxo-3yfwjF6"
      },
      "source": [
        "### 3.3. Similar Bug Reports  \n",
        "Dựa trên các bug reports trước đó mà source file đã được fix, xem xem có giống các bug report đã từng sửa cùng file đó không"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQF0v-GQfe5a"
      },
      "outputs": [],
      "source": [
        "def find_source_files_for_bug_report(target_bug_report, dataset):\n",
        "    # Tìm tất cả các SourceFile liên quan đến BugReport\n",
        "    source_files = [source_file for bug_report, source_file, label in dataset if bug_report == target_bug_report]\n",
        "    return source_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpJzPhH_hZ6I"
      },
      "outputs": [],
      "source": [
        "test_src_files = find_source_files_for_bug_report(data_train[2001][0], data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAPCWPHNhUgt"
      },
      "outputs": [],
      "source": [
        "def find_bug_reports_fix_source_files(target_source_files, dataset):\n",
        "    # Tìm tất cả BugReport đã fix SourceFiles (Bug Report đã sửa file đó)(label = 1)\n",
        "    bug_reports = [bug_report for bug_report, source_file, label in dataset if source_file in target_source_files and label == 1]\n",
        "    return bug_reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap5envP9igBj",
        "outputId": "b46842ec-048a-4ce5-8f8f-fdb0a0401939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49\n"
          ]
        }
      ],
      "source": [
        "print(len(find_bug_reports_fix_source_files(test_src_files, data_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHm3EXXdj_8O"
      },
      "outputs": [],
      "source": [
        "def get_previous_bug_reports(bug_report, dataset):\n",
        "  source_files = find_source_files_for_bug_report(bug_report, dataset)\n",
        "  bug_reports = find_bug_reports_fix_source_files(source_files, dataset)\n",
        "  return bug_reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECgFOAv9kxER"
      },
      "outputs": [],
      "source": [
        "def get_similar_bugs_score(pair, dataset):\n",
        "  #pair (BugReport, SrcFile, Label)\n",
        "  previous_bug_reports = get_previous_bug_reports(pair[0], dataset)\n",
        "  if len(previous_bug_reports) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    sum = 0\n",
        "    for bug_report in previous_bug_reports:\n",
        "      bug_report_text = bug_report_to_text(pair[0])\n",
        "      bug2_report_text = bug_report_to_text(bug_report)\n",
        "      # Kết hợp văn bản của bug_report và bug2_report\n",
        "      all_texts = [bug_report_text, bug2_report_text]\n",
        "\n",
        "      # Tính TF-IDF cho văn bản\n",
        "      vectorizer = TfidfVectorizer()\n",
        "      tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "      # Lấy các vector TF-IDF cho bug_report và bug2_report\n",
        "      bug_report_tfidf = tfidf_matrix[0]  # Vector của bug_report\n",
        "      bug2_report_tfidf = tfidf_matrix[1]  # Vector của bug2_report\n",
        "\n",
        "      # Tính cosine similarity giữa bug_report và bug2_report\n",
        "      similarity = cosine_similarity(bug_report_tfidf, bug2_report_tfidf)\n",
        "      sum += similarity[0][0]\n",
        "\n",
        "    return sum / len(previous_bug_reports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwX5cREFnFpn",
        "outputId": "11aa16b4-7a35-4687-f5b7-18d121be5e6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.19250472938339938)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_similar_bugs_score(data_train[2001], data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw47Eyu1nkEs"
      },
      "source": [
        "### 3.4. Code change history\n",
        "File mà đã fix rất lâu về trước hoặc chưa bao giờ fix thì sẽ ít khả năng lỗi hơn\n",
        "- Trả về 1 / khoảng cách ngày tới lần fix gần nhất\n",
        " --> Nếu như khoảng cách càng xa, score càng bé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRPd5ddCo3Bw"
      },
      "outputs": [],
      "source": [
        "def get_code_change_history_score(pair, dataset):\n",
        "  report_time = pair[0].report_time\n",
        "  # Trả về tất cả các bug reports mà đã fix source file\n",
        "  fixed_bug_reports_for_src_file = [bug_report for bug_report, source_file, label in dataset if source_file == pair[1] and label == 1]\n",
        "  min_time = report_time\n",
        "  # Lấy report time của các bug_reports đã fix xong, nếu nhỏ hơn report time thì gán min time vào\n",
        "  for bug_report in fixed_bug_reports_for_src_file:\n",
        "    if bug_report.report_time < report_time:\n",
        "      min_time = min(min_time, bug_report.report_time)\n",
        "\n",
        "  elapsed_days = (report_time - min_time).days\n",
        "  if elapsed_days == 0:\n",
        "    return 1 #Nếu trả về 1 thì tức là file đó chưa từng được fix lần nào trước report time của bug_report\n",
        "  else:\n",
        "    return 1 / elapsed_days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHa9IItjGNmr",
        "outputId": "7dd0b1ff-2bdc-4498-df0b-899fc657c826"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_code_change_history_score(data_train[2001], data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8-rrsTxGWfp"
      },
      "source": [
        "### 3.5. Bug fixing frequency\n",
        "Số lần mà source file được sửa trước report time cũng sẽ ảnh hưởng đến xác suất file đó có lỗi hay không  \n",
        "(Nếu như sửa càng nhiều --> xác suất lỗi càng cao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hON3jKglGlqK"
      },
      "outputs": [],
      "source": [
        "def get_bug_fixing_freq_score(pair, dataset):\n",
        "  report_time = pair[0].report_time\n",
        "  # Trả về tất cả các bug reports mà đã fix source file\n",
        "  fixed_bug_reports_for_src_file = [bug_report for bug_report, source_file, label in dataset\n",
        "                                    if source_file == pair[1] and label == 1]\n",
        "  # Tìm trong các bug_report đã fix nếu report time nhỏ hơn bug_report hiện tại (hay đã fix trước đó)\n",
        "  count = 0\n",
        "  for bug_report in fixed_bug_reports_for_src_file:\n",
        "    if bug_report.report_time < report_time:\n",
        "      count +=1\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9mjafZlHcS3",
        "outputId": "2caa8ea0-59a1-4342-f980-cbffea596414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_bug_fixing_freq_score(data_train[2001], data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZOHcw_oHpwT"
      },
      "source": [
        "## 4. Tạo tập train và tập test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ4lYC2Hz7J"
      },
      "source": [
        "### Gộp các features đã tính\n",
        "Gộp các features đã tính thành 1 vector `[lex_sim, sem_sim, prev_bug_sim, change_history, change_freq]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHv7bC6iHy22"
      },
      "outputs": [],
      "source": [
        "def extract_features(pair, dataset, glove_dict):\n",
        "    lex_sim = get_lexical_similarity(pair)\n",
        "    sem_sim = get_semantic_similarity(pair, glove_dict)\n",
        "    prev_bug_sim = get_similar_bugs_score(pair, dataset)\n",
        "    change_history = get_code_change_history_score(pair, dataset)\n",
        "    change_freq = get_bug_fixing_freq_score(pair, dataset)\n",
        "\n",
        "    features_vector = np.array([lex_sim, sem_sim, prev_bug_sim, change_history, change_freq])\n",
        "    return features_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXZYqTDLIbAC",
        "outputId": "3ebb5407-04f4-4830-e49e-8e23c62ecdc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.24505498, 0.86022142, 0.02045434, 1.        , 0.        ])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_features(data_train[0], data_train, glove_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hàm tạo data_train, data_test"
      ],
      "metadata": {
        "id": "asTeu2S0-Dm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYccBBDpJCTP"
      },
      "outputs": [],
      "source": [
        "# Hàm dùng để tạo data: data có dạng [(vector, label),....]\n",
        "def create_data_train(dataset, glove_dict):\n",
        "  print(f\"Creating training data for {len(dataset)} samples\")\n",
        "  data = []\n",
        "  for pair in dataset:  #pair: (bug_report, src_file, label) --> label: pair[2]\n",
        "    x_pair = extract_features(pair, dataset, glove_dict)\n",
        "    y_pair = pair[2]\n",
        "    data.append((x_pair, y_pair))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKDWB1K_-j6T"
      },
      "outputs": [],
      "source": [
        "# Group lại theo bug report {bug_report: [(feature_vector, label), ...]} (data_test)\n",
        "from collections import defaultdict\n",
        "def create_data_test(dataset, glove_dict):\n",
        "    print(f\"Creating testing data for {len(dataset)} samples\")\n",
        "    grouped_data = defaultdict(list)  # {bug_report: [(feature_vector, label), ...]}\n",
        "\n",
        "    for bug_report, src_file, label in dataset:\n",
        "        pair = (bug_report, src_file, label)\n",
        "        feature_vector = extract_features(pair, dataset, glove_dict)\n",
        "        grouped_data[bug_report].append((feature_vector, label))\n",
        "\n",
        "    return grouped_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dboyu69lhZP-"
      },
      "source": [
        "### Tạo minibatch cho dữ liệu train bằng Bootstraping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zacDLg86LIJM"
      },
      "outputs": [],
      "source": [
        "# Lấy ra tập (vector, label) positive\n",
        "def get_pos(data):\n",
        "  pos_set = []\n",
        "  for vector, label in data:\n",
        "    if label == 1:\n",
        "      pos_set.append((vector, 1))\n",
        "  return pos_set\n",
        "\n",
        "# Lấy ra tập (vector, label) negative\n",
        "def get_neg(data):\n",
        "  neg_set = []\n",
        "  for vector, label in data:\n",
        "    if label == 0:\n",
        "      neg_set.append((vector, 0))\n",
        "  return neg_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMxRUgiYI15K"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Cho tập train\n",
        "def create_minibatch(data, batch_size):\n",
        "  print(f\"Creating minibatch for {len(data)} samples\")\n",
        "  mini_batches = []\n",
        "  pos_set = get_pos(data) #[(X_pos, 1)]\n",
        "  neg_set = get_neg(data) #[(X_neg, 0)]\n",
        "\n",
        "  n_pos = len(pos_set)\n",
        "  n_neg = len(neg_set)\n",
        "  K = n_neg // batch_size\n",
        "\n",
        "  Sn = batch_size // 2\n",
        "  Sp = batch_size - Sn\n",
        "\n",
        "  #Chia negative thành K phần, từng phần có Sn samples\n",
        "  neg_batches = [neg_set[i * Sn : (i + 1) * Sn] for i in range(K)]\n",
        "\n",
        "  #Copy tập postive để rút ngẫu nhiên\n",
        "  temp_pos = pos_set.copy()\n",
        "\n",
        "  for i in range(K):\n",
        "    if len(temp_pos) < Sp:\n",
        "      temp_pos = pos_set.copy()\n",
        "\n",
        "    random.shuffle(temp_pos)\n",
        "    #pos_i = random.sample(temp_pos, Sp)\n",
        "    pos_i = random.choices(temp_pos, k=Sp)\n",
        "\n",
        "    #for item in pos_i:\n",
        "    #  temp_pos.remove(item)\n",
        "\n",
        "\n",
        "\n",
        "    #batch = neg_batches[i] + pos_i\n",
        "    #random.shuffle(batch)\n",
        "    batch_inputs = [x for x, y in neg_batches[i] + pos_i]\n",
        "    batch_labels = [y for x, y in neg_batches[i] + pos_i]\n",
        "\n",
        "    mini_batches.append((batch_inputs, batch_labels))\n",
        "  return mini_batches # 1 batch[(X,y),...]; 1 minibatch: list các batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAnkoop2Mvn4"
      },
      "source": [
        "## 5. Huấn luyện và đánh giá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96jGtYjBPBS2"
      },
      "source": [
        "### Định nghĩa mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPaVtgshPEJh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Định nghĩa hàm focal loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "\n",
        "# Mô hình DNN\n",
        "class BugLocalization(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BugLocalization, self).__init__()\n",
        "        # Các lớp ẩn với kích thước 300 và 150 node\n",
        "        self.fc1 = nn.Linear(input_dim, 300)\n",
        "        self.fc2 = nn.Linear(300, 150)\n",
        "        self.fc3 = nn.Linear(150, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiIkf7YOPFKh"
      },
      "source": [
        "### Huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dxWl_0nMyqx"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84saq-nxTdCD"
      },
      "outputs": [],
      "source": [
        "def train_model(model, mini_batches, criterion, optimizer, device, project_name, epochs):\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    save_dir = f\"{models_dir}/{project_name}_training_model\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in mini_batches:\n",
        "            batch_inputs = torch.tensor(batch_inputs, dtype=torch.float32).to(device)\n",
        "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs)\n",
        "            loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            #In ra mỗi 50 batch\n",
        "            if batch_count % 50 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{epochs}], Batch [{batch_count}/{len(mini_batches)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(mini_batches)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pt\")\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss,\n",
        "        }\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Model checkpoint saved at {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO8lCX5k_xGf"
      },
      "source": [
        "### Đánh giá mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVyb7n4VuXd"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def evaluate_model(model, data_test, criterion, device, k_values=[1, 5, 10, 15]):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    top_k_hits = defaultdict(int)\n",
        "    total_reports = 0\n",
        "    mrr_total = 0.0\n",
        "    map_total = 0.0\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for bug_report, samples in data_test.items():\n",
        "          if not samples:\n",
        "            continue\n",
        "\n",
        "          total_reports += 1\n",
        "          vectors = [vec for vec, _ in samples]\n",
        "          labels = [label for _, label in samples]\n",
        "\n",
        "          vectors = torch.tensor(vectors, dtype=torch.float32).to(device)\n",
        "          labels = np.array(labels)\n",
        "\n",
        "          outputs = model(vectors).view(-1)\n",
        "\n",
        "          y_pred = outputs.cpu().numpy()\n",
        "          loss = criterion(outputs, torch.tensor(labels, dtype=torch.float32).to(device)).item()\n",
        "          total_loss += loss * len(samples)\n",
        "          total_samples += len(samples)\n",
        "\n",
        "          ranked = sorted(zip(y_pred, labels), key=lambda x: x[0], reverse=True)\n",
        "          ranked_labels = [label for _, label in ranked]\n",
        "\n",
        "          #Tính top k accuracy\n",
        "          for k in k_values:\n",
        "            top_k = ranked_labels[:k]\n",
        "            if 1 in top_k:\n",
        "              top_k_hits[k] += 1\n",
        "\n",
        "          #Tính MRR\n",
        "          for i, label in enumerate(ranked_labels):\n",
        "            if label == 1:\n",
        "              mrr_total += 1 / (i + 1)\n",
        "              break\n",
        "\n",
        "          #Tính MAP\n",
        "          num_correct = 0\n",
        "          precisions = []\n",
        "          for i, label in enumerate(ranked_labels):\n",
        "            if label == 1:\n",
        "              num_correct += 1\n",
        "              precision = num_correct / (i + 1)\n",
        "              precisions.append(precision)\n",
        "          if precisions:\n",
        "            map_total += sum(precisions) / num_correct\n",
        "\n",
        "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
        "    results = {\n",
        "        'loss': avg_loss,\n",
        "        'top_k_accuracy': OrderedDict((k, top_k_hits[k] / total_reports) for k in k_values),\n",
        "        'mrr': mrr_total / total_reports,\n",
        "        'map': map_total / total_reports\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkweXOt2YwO7"
      },
      "outputs": [],
      "source": [
        "model = BugLocalization(input_dim=5)\n",
        "criterion = FocalLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgeJD2n5_oAz"
      },
      "source": [
        "### Lấy 1 phần để thử pipeline(test hàm train, evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhoZgQafb_H0"
      },
      "outputs": [],
      "source": [
        "#Sample 1 phần của set mà vẫn giữ phân phối\n",
        "import random\n",
        "def stratified(data, sample_size, min_label_1 = 1):\n",
        "    data_label_1 = [d for d in data if d[2] == 1]\n",
        "    data_label_0 = [d for d in data if d[2] == 0]\n",
        "\n",
        "    total = len(data)\n",
        "    ratio_label_1 = len(data_label_1) / total\n",
        "\n",
        "    # Tính số mẫu cần lấy\n",
        "    num_label_1 = max(int(sample_size * ratio_label_1), min_label_1)\n",
        "    num_label_1 = min(num_label_1, len(data_label_1))  # không vượt quá số thực tế\n",
        "    num_label_0 = sample_size - num_label_1\n",
        "\n",
        "    sampled_label_1 = random.sample(data_label_1, num_label_1)\n",
        "    sampled_label_0 = random.sample(data_label_0, num_label_0)\n",
        "\n",
        "    sampled_data = sampled_label_1 + sampled_label_0\n",
        "    random.shuffle(sampled_data)\n",
        "    return sampled_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_temp = stratified(data_train,5000)\n",
        "data_test_temp = stratified(data_test,5000)\n",
        "\n",
        "data_train_temp = create_data_train(data_train_temp, glove_model)\n",
        "data_test_temp = create_data_test(data_test_temp, glove_model)"
      ],
      "metadata": {
        "id": "w9Xkoj-TnWZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yYDhnn0qLPW",
        "outputId": "836cb7ab-5cb9-47d4-e5e4-b55564dc6bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating minibatch for 5000 samples\n",
            "---------------------------------------------\n",
            "\n",
            "TRAINING\n",
            "Epoch [1/30], Batch [50/74], Loss: 0.0440\n",
            "Epoch [1/30], Loss: 0.0446\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_1.pt\n",
            "Epoch [2/30], Batch [50/74], Loss: 0.0446\n",
            "Epoch [2/30], Loss: 0.0443\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_2.pt\n",
            "Epoch [3/30], Batch [50/74], Loss: 0.0449\n",
            "Epoch [3/30], Loss: 0.0442\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_3.pt\n",
            "Epoch [4/30], Batch [50/74], Loss: 0.0433\n",
            "Epoch [4/30], Loss: 0.0441\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_4.pt\n",
            "Epoch [5/30], Batch [50/74], Loss: 0.0442\n",
            "Epoch [5/30], Loss: 0.0439\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_5.pt\n",
            "Epoch [6/30], Batch [50/74], Loss: 0.0434\n",
            "Epoch [6/30], Loss: 0.0438\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_6.pt\n",
            "Epoch [7/30], Batch [50/74], Loss: 0.0436\n",
            "Epoch [7/30], Loss: 0.0438\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_7.pt\n",
            "Epoch [8/30], Batch [50/74], Loss: 0.0427\n",
            "Epoch [8/30], Loss: 0.0436\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_8.pt\n",
            "Epoch [9/30], Batch [50/74], Loss: 0.0432\n",
            "Epoch [9/30], Loss: 0.0435\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_9.pt\n",
            "Epoch [10/30], Batch [50/74], Loss: 0.0432\n",
            "Epoch [10/30], Loss: 0.0434\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_10.pt\n",
            "Epoch [11/30], Batch [50/74], Loss: 0.0436\n",
            "Epoch [11/30], Loss: 0.0433\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_11.pt\n",
            "Epoch [12/30], Batch [50/74], Loss: 0.0431\n",
            "Epoch [12/30], Loss: 0.0432\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_12.pt\n",
            "Epoch [13/30], Batch [50/74], Loss: 0.0437\n",
            "Epoch [13/30], Loss: 0.0433\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_13.pt\n",
            "Epoch [14/30], Batch [50/74], Loss: 0.0420\n",
            "Epoch [14/30], Loss: 0.0432\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_14.pt\n",
            "Epoch [15/30], Batch [50/74], Loss: 0.0435\n",
            "Epoch [15/30], Loss: 0.0431\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_15.pt\n",
            "Epoch [16/30], Batch [50/74], Loss: 0.0422\n",
            "Epoch [16/30], Loss: 0.0430\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_16.pt\n",
            "Epoch [17/30], Batch [50/74], Loss: 0.0434\n",
            "Epoch [17/30], Loss: 0.0429\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_17.pt\n",
            "Epoch [18/30], Batch [50/74], Loss: 0.0427\n",
            "Epoch [18/30], Loss: 0.0429\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_18.pt\n",
            "Epoch [19/30], Batch [50/74], Loss: 0.0426\n",
            "Epoch [19/30], Loss: 0.0429\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_19.pt\n",
            "Epoch [20/30], Batch [50/74], Loss: 0.0440\n",
            "Epoch [20/30], Loss: 0.0427\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_20.pt\n",
            "Epoch [21/30], Batch [50/74], Loss: 0.0418\n",
            "Epoch [21/30], Loss: 0.0425\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_21.pt\n",
            "Epoch [22/30], Batch [50/74], Loss: 0.0424\n",
            "Epoch [22/30], Loss: 0.0425\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_22.pt\n",
            "Epoch [23/30], Batch [50/74], Loss: 0.0426\n",
            "Epoch [23/30], Loss: 0.0425\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_23.pt\n",
            "Epoch [24/30], Batch [50/74], Loss: 0.0425\n",
            "Epoch [24/30], Loss: 0.0424\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_24.pt\n",
            "Epoch [25/30], Batch [50/74], Loss: 0.0420\n",
            "Epoch [25/30], Loss: 0.0425\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_25.pt\n",
            "Epoch [26/30], Batch [50/74], Loss: 0.0420\n",
            "Epoch [26/30], Loss: 0.0423\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_26.pt\n",
            "Epoch [27/30], Batch [50/74], Loss: 0.0428\n",
            "Epoch [27/30], Loss: 0.0423\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_27.pt\n",
            "Epoch [28/30], Batch [50/74], Loss: 0.0418\n",
            "Epoch [28/30], Loss: 0.0421\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_28.pt\n",
            "Epoch [29/30], Batch [50/74], Loss: 0.0430\n",
            "Epoch [29/30], Loss: 0.0422\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_29.pt\n",
            "Epoch [30/30], Batch [50/74], Loss: 0.0421\n",
            "Epoch [30/30], Loss: 0.0421\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_30.pt\n"
          ]
        }
      ],
      "source": [
        "mini_batches = create_minibatch(data_train_temp, batch_size)\n",
        "#Training\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TRAINING\")\n",
        "train_model(model, mini_batches, criterion, optimizer, device, 'tempo', epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaehEuS0CIR",
        "outputId": "5ea05f51-0bc8-407c-cdee-2dace8b35c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------\n",
            "\n",
            "TESTING\n",
            "{'loss': 0.04255290679857135, 'top_k_accuracy': {1: 0.20707070707070707, 5: 0.29292929292929293, 10: 0.3333333333333333, 15: 0.35858585858585856}, 'mrr': 0.24733852038374127, 'map': 0.2322692084715962}\n"
          ]
        }
      ],
      "source": [
        "#Testing\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TESTING\")\n",
        "results = evaluate_model(model, data_test_temp, criterion, device)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzCAhTGy_jhn"
      },
      "source": [
        "### Cross-validation cho project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKmIqJmZiG9k"
      },
      "outputs": [],
      "source": [
        "#Tạo sẵn data_train, data_test và minibatch (Đã chuyển về dạng (vector, label))\n",
        "def prepare_data(project_name, batch_size, num_folds, glove_model):\n",
        "  for i in range(num_folds-1):\n",
        "    data_dir = f\"{models_dir}/{project_name}/data_train_test\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    data_train, data_test = load_fold_data('aspectj', i)\n",
        "    data_train = create_data_train(data_train, glove_model)\n",
        "    print(\"Saving train data\")\n",
        "    with open(os.path.join(data_dir, f\"fold_{i}_data_train.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(data_train, f)\n",
        "\n",
        "    data_test = create_data_test(data_test, glove_model)\n",
        "    print(\"Saving test data\")\n",
        "    with open(os.path.join(data_dir, f\"fold_{i}_data_test.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(data_test, f)\n",
        "\n",
        "    mini_batches = create_minibatch(data_train, batch_size)\n",
        "    print(\"Saving minibatches\")\n",
        "    with open(os.path.join(data_dir, f\"fold_{i}_mini_batches.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(mini_batches, f)\n",
        "\n",
        "    print(f\"Saved fold {i}: train={len(data_train)}, test={len(data_test)}, mini_batches={len(mini_batches)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data('aspectj', batch_size, 3, glove_model)"
      ],
      "metadata": {
        "id": "AOWkzH7GpY2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwb7eqSfkb8u"
      },
      "outputs": [],
      "source": [
        "def load_prepared_data(project_name, fold_idx):\n",
        "    data_dir = f\"{models_dir}/{project_name}/data_train_test\"\n",
        "\n",
        "    data_train = os.path.join(data_dir, f\"fold_{fold_idx}_data_train.pkl\")\n",
        "    data_test = os.path.join(data_dir, f\"fold_{fold_idx}_data_test.pkl\")\n",
        "    mini_batches = os.path.join(data_dir, f\"fold_{fold_idx}_mini_batches.pkl\")\n",
        "\n",
        "    # Load tập train, tập test\n",
        "    with open(data_train, \"rb\") as f:\n",
        "        data_train = pickle.load(f)\n",
        "    with open(data_test, \"rb\") as f:\n",
        "        data_test = pickle.load(f)\n",
        "    with open(mini_batches, \"rb\") as f:\n",
        "        mini_batches = pickle.load(f)\n",
        "    return data_train, data_test, mini_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hto2ySEM2IR"
      },
      "outputs": [],
      "source": [
        "def cross_validation(project_name, k_fold, model, criterion, optimizer, device, batch_size, epochs):\n",
        "  for i in range(k_fold - 1):\n",
        "    load_prepared_data(project_name, i)\n",
        "    #Training\n",
        "    print(\"---------------------------------------------\\n\")\n",
        "    print(f\"TRAINING ON FOLD {i}\")\n",
        "    train_model(model, mini_batches, criterion, optimizer, device, project_name, epochs)\n",
        "    #Testing\n",
        "    print(\"---------------------------------------------\\n\")\n",
        "    print(f\"TESTING ON FOLD {i+1}\")\n",
        "    results = evaluate_model(model, data_test, criterion, device)\n",
        "    print(f\"Fold{i} -: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ewwWKyjqb0H1",
        "outputId": "a292412b-3352-4058-c4ca-6966810e85d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating data for 99523 samples\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-246-19adc31e814c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aspectj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-245-2e713405fb09>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(project_name, k_fold, model, criterion, optimizer, device, batch_size, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#data_test = stratified(data_test, 10000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Tạo data sau khi đã xử lý features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data_train [(vector, label),....]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[(vector, label),....]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 list các batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-0d555ab6e073>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(dataset, glove_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#pair: (bug_report, src_file, label) --> label: pair[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-167a0d364774>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(pair, dataset, glove_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlex_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lexical_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msem_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprev_bug_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_bugs_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchange_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_code_change_history_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mchange_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bug_fixing_freq_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-7e8743852bc1>\u001b[0m in \u001b[0;36mget_similar_bugs_score\u001b[0;34m(pair, dataset)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m# Tính TF-IDF cho văn bản\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# Lấy các vector TF-IDF cho bug_report và bug2_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;31m# we set copy to False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;31m# _document_frequency uses np.bincount which works on arrays of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m         \u001b[0;31m# dtype NPY_INTP which is int32 for 32bit platforms. See #20923\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2918\u001b[0m     \"\"\"\n\u001b[1;32m   2919\u001b[0m     \u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2920\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0mtag_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_tags_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtag_provider\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36m_find_tags_provider\u001b[0;34m(estimator, warn)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mtag_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_mro\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         has_get_or_more_tags = any(\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mprovider\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprovider\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"_get_tags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mtag_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_mro\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         has_get_or_more_tags = any(\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mprovider\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprovider\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"_get_tags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cross_validation('aspectj', 3, model, criterion, optimizer, device, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, _ = load_fold_data('aspectj', 0)\n",
        "data_test, _ = load_fold_data('aspectj', 1)\n",
        "train_fold_0 = create_data_train(data_train, glove_model)\n",
        "test_fold_0 = create_data_test(data_test, glove_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys1-NPqlpZ5p",
        "outputId": "0f5c61f8-f70c-4f8a-b22f-4519a89935a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data for 10423 samples\n",
            "Creating testing data for 10168 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batches = create_minibatch(train_fold_0, batch_size)\n",
        "#Training\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TRAINING\")\n",
        "train_model(model, mini_batches, criterion, optimizer, device, 'tempo', epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw7iF1txqV8S",
        "outputId": "f66a96a5-b7b9-4e82-9a03-10204ae865a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating minibatch for 10423 samples\n",
            "---------------------------------------------\n",
            "\n",
            "TRAINING\n",
            "Epoch [1/30], Batch [50/154], Loss: 0.1739\n",
            "Epoch [1/30], Batch [100/154], Loss: 0.1731\n",
            "Epoch [1/30], Batch [150/154], Loss: 0.1711\n",
            "Epoch [1/30], Loss: 0.1733\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_1.pt\n",
            "Epoch [2/30], Batch [50/154], Loss: 0.1728\n",
            "Epoch [2/30], Batch [100/154], Loss: 0.1793\n",
            "Epoch [2/30], Batch [150/154], Loss: 0.1711\n",
            "Epoch [2/30], Loss: 0.1728\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_2.pt\n",
            "Epoch [3/30], Batch [50/154], Loss: 0.1781\n",
            "Epoch [3/30], Batch [100/154], Loss: 0.1757\n",
            "Epoch [3/30], Batch [150/154], Loss: 0.1713\n",
            "Epoch [3/30], Loss: 0.1724\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_3.pt\n",
            "Epoch [4/30], Batch [50/154], Loss: 0.1712\n",
            "Epoch [4/30], Batch [100/154], Loss: 0.1690\n",
            "Epoch [4/30], Batch [150/154], Loss: 0.1691\n",
            "Epoch [4/30], Loss: 0.1718\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_4.pt\n",
            "Epoch [5/30], Batch [50/154], Loss: 0.1726\n",
            "Epoch [5/30], Batch [100/154], Loss: 0.1710\n",
            "Epoch [5/30], Batch [150/154], Loss: 0.1770\n",
            "Epoch [5/30], Loss: 0.1714\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_5.pt\n",
            "Epoch [6/30], Batch [50/154], Loss: 0.1734\n",
            "Epoch [6/30], Batch [100/154], Loss: 0.1721\n",
            "Epoch [6/30], Batch [150/154], Loss: 0.1659\n",
            "Epoch [6/30], Loss: 0.1714\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_6.pt\n",
            "Epoch [7/30], Batch [50/154], Loss: 0.1758\n",
            "Epoch [7/30], Batch [100/154], Loss: 0.1668\n",
            "Epoch [7/30], Batch [150/154], Loss: 0.1705\n",
            "Epoch [7/30], Loss: 0.1706\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_7.pt\n",
            "Epoch [8/30], Batch [50/154], Loss: 0.1710\n",
            "Epoch [8/30], Batch [100/154], Loss: 0.1730\n",
            "Epoch [8/30], Batch [150/154], Loss: 0.1689\n",
            "Epoch [8/30], Loss: 0.1706\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_8.pt\n",
            "Epoch [9/30], Batch [50/154], Loss: 0.1701\n",
            "Epoch [9/30], Batch [100/154], Loss: 0.1690\n",
            "Epoch [9/30], Batch [150/154], Loss: 0.1675\n",
            "Epoch [9/30], Loss: 0.1704\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_9.pt\n",
            "Epoch [10/30], Batch [50/154], Loss: 0.1735\n",
            "Epoch [10/30], Batch [100/154], Loss: 0.1705\n",
            "Epoch [10/30], Batch [150/154], Loss: 0.1694\n",
            "Epoch [10/30], Loss: 0.1696\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_10.pt\n",
            "Epoch [11/30], Batch [50/154], Loss: 0.1709\n",
            "Epoch [11/30], Batch [100/154], Loss: 0.1714\n",
            "Epoch [11/30], Batch [150/154], Loss: 0.1719\n",
            "Epoch [11/30], Loss: 0.1694\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_11.pt\n",
            "Epoch [12/30], Batch [50/154], Loss: 0.1684\n",
            "Epoch [12/30], Batch [100/154], Loss: 0.1687\n",
            "Epoch [12/30], Batch [150/154], Loss: 0.1686\n",
            "Epoch [12/30], Loss: 0.1690\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_12.pt\n",
            "Epoch [13/30], Batch [50/154], Loss: 0.1713\n",
            "Epoch [13/30], Batch [100/154], Loss: 0.1671\n",
            "Epoch [13/30], Batch [150/154], Loss: 0.1651\n",
            "Epoch [13/30], Loss: 0.1684\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_13.pt\n",
            "Epoch [14/30], Batch [50/154], Loss: 0.1720\n",
            "Epoch [14/30], Batch [100/154], Loss: 0.1698\n",
            "Epoch [14/30], Batch [150/154], Loss: 0.1691\n",
            "Epoch [14/30], Loss: 0.1685\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_14.pt\n",
            "Epoch [15/30], Batch [50/154], Loss: 0.1635\n",
            "Epoch [15/30], Batch [100/154], Loss: 0.1707\n",
            "Epoch [15/30], Batch [150/154], Loss: 0.1669\n",
            "Epoch [15/30], Loss: 0.1682\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_15.pt\n",
            "Epoch [16/30], Batch [50/154], Loss: 0.1681\n",
            "Epoch [16/30], Batch [100/154], Loss: 0.1680\n",
            "Epoch [16/30], Batch [150/154], Loss: 0.1665\n",
            "Epoch [16/30], Loss: 0.1680\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_16.pt\n",
            "Epoch [17/30], Batch [50/154], Loss: 0.1670\n",
            "Epoch [17/30], Batch [100/154], Loss: 0.1690\n",
            "Epoch [17/30], Batch [150/154], Loss: 0.1677\n",
            "Epoch [17/30], Loss: 0.1676\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_17.pt\n",
            "Epoch [18/30], Batch [50/154], Loss: 0.1696\n",
            "Epoch [18/30], Batch [100/154], Loss: 0.1694\n",
            "Epoch [18/30], Batch [150/154], Loss: 0.1682\n",
            "Epoch [18/30], Loss: 0.1676\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_18.pt\n",
            "Epoch [19/30], Batch [50/154], Loss: 0.1693\n",
            "Epoch [19/30], Batch [100/154], Loss: 0.1671\n",
            "Epoch [19/30], Batch [150/154], Loss: 0.1649\n",
            "Epoch [19/30], Loss: 0.1668\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_19.pt\n",
            "Epoch [20/30], Batch [50/154], Loss: 0.1689\n",
            "Epoch [20/30], Batch [100/154], Loss: 0.1747\n",
            "Epoch [20/30], Batch [150/154], Loss: 0.1622\n",
            "Epoch [20/30], Loss: 0.1665\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_20.pt\n",
            "Epoch [21/30], Batch [50/154], Loss: 0.1695\n",
            "Epoch [21/30], Batch [100/154], Loss: 0.1654\n",
            "Epoch [21/30], Batch [150/154], Loss: 0.1675\n",
            "Epoch [21/30], Loss: 0.1668\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_21.pt\n",
            "Epoch [22/30], Batch [50/154], Loss: 0.1670\n",
            "Epoch [22/30], Batch [100/154], Loss: 0.1673\n",
            "Epoch [22/30], Batch [150/154], Loss: 0.1608\n",
            "Epoch [22/30], Loss: 0.1663\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_22.pt\n",
            "Epoch [23/30], Batch [50/154], Loss: 0.1681\n",
            "Epoch [23/30], Batch [100/154], Loss: 0.1681\n",
            "Epoch [23/30], Batch [150/154], Loss: 0.1592\n",
            "Epoch [23/30], Loss: 0.1660\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_23.pt\n",
            "Epoch [24/30], Batch [50/154], Loss: 0.1685\n",
            "Epoch [24/30], Batch [100/154], Loss: 0.1655\n",
            "Epoch [24/30], Batch [150/154], Loss: 0.1624\n",
            "Epoch [24/30], Loss: 0.1657\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_24.pt\n",
            "Epoch [25/30], Batch [50/154], Loss: 0.1660\n",
            "Epoch [25/30], Batch [100/154], Loss: 0.1640\n",
            "Epoch [25/30], Batch [150/154], Loss: 0.1606\n",
            "Epoch [25/30], Loss: 0.1653\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_25.pt\n",
            "Epoch [26/30], Batch [50/154], Loss: 0.1630\n",
            "Epoch [26/30], Batch [100/154], Loss: 0.1634\n",
            "Epoch [26/30], Batch [150/154], Loss: 0.1624\n",
            "Epoch [26/30], Loss: 0.1650\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_26.pt\n",
            "Epoch [27/30], Batch [50/154], Loss: 0.1666\n",
            "Epoch [27/30], Batch [100/154], Loss: 0.1640\n",
            "Epoch [27/30], Batch [150/154], Loss: 0.1652\n",
            "Epoch [27/30], Loss: 0.1654\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_27.pt\n",
            "Epoch [28/30], Batch [50/154], Loss: 0.1691\n",
            "Epoch [28/30], Batch [100/154], Loss: 0.1673\n",
            "Epoch [28/30], Batch [150/154], Loss: 0.1595\n",
            "Epoch [28/30], Loss: 0.1648\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_28.pt\n",
            "Epoch [29/30], Batch [50/154], Loss: 0.1644\n",
            "Epoch [29/30], Batch [100/154], Loss: 0.1653\n",
            "Epoch [29/30], Batch [150/154], Loss: 0.1595\n",
            "Epoch [29/30], Loss: 0.1645\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_29.pt\n",
            "Epoch [30/30], Batch [50/154], Loss: 0.1703\n",
            "Epoch [30/30], Batch [100/154], Loss: 0.1630\n",
            "Epoch [30/30], Batch [150/154], Loss: 0.1642\n",
            "Epoch [30/30], Loss: 0.1646\n",
            "Model checkpoint saved at drive/MyDrive/Colab Notebooks/BugLocalization/models/tempo_training_model/epoch_30.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "print(\"---------------------------------------------\\n\")\n",
        "print(f\"TESTING\")\n",
        "results = evaluate_model(model, test_fold_0, criterion, device)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LotQ_SRLqYE5",
        "outputId": "5a00f5b4-cb6a-40d2-b8ce-604e3d3cde5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "\n",
            "TESTING\n",
            "{'loss': 0.17083648026405734, 'top_k_accuracy': {1: 0.18181818181818182, 5: 0.3939393939393939, 10: 0.5303030303030303, 15: 0.5555555555555556}, 'mrr': 0.2762475038716552, 'map': 0.25491307594064794}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cgeJD2n5_oAz"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1P6_P-hX6fY3zwcxsTW8aa7t_-S-3RTlw",
      "authorship_tag": "ABX9TyMcxdw3Rq94Dlt86oP0oGly"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}